{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Essa tarefa tem como objetivo demonstrar uma Pipeline de Supervisão Fraca.<br>\n",
    "Para essa tarefa eu escolhi utilizar o conjunto de dados da minha primeira tarefa no meu estágio, que comecei a um ano atrás.<br>\n",
    "Essa tarefa tem o objetivo encontrar as espécies, o tipo do corte e as dimensões das descrições de madeira.<br>\n",
    "Além de ser um trabalho de MLP, que na época eu não tinha nenhuma experiência, também teve a grande dificuldade dos dados não estarem rotulados.<br>\n",
    "A minha solução final envolveu a construção de um conjunto de treino e validação manual e utilizar uma LLM com um schema definido para retornar a espécie, o corte e as dimensões.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Temos algumas características que tornaram esse problema difícil para uma LLM, o primeiro, as descrições de produto não são estruturadas, além de não existir nenhum tipo de ordem para cada uma das classes de interesse também podemos ter casos em que essas classes estão faltando, como por exemplo uma descrição possuir a espécie, o corte, mas não possuir a dimensão.<br>\n",
    "Exemplo: \"tabua de massaranduba.\"<br>\n",
    "Um outro problema são as situações em que uma descrição possui múltiplos de uma mesma classe.<br>\n",
    "Exemplo: \"tabua/viga de massaranduba.\"<br>\n",
    "Um problema mais simples são erros ortográficos, sinônimos e nomes ciêntificos da espécie, existem múltiplas maneiras de chamar um mesmo produto que faz sentido no contexto de produtos de madeira, mas são palavras completamente diferentes em outros contextos.<br>\n",
    "Exemplo1: \"tabua de cetim.\"<br>\n",
    "Exemplo2: \"tabua de pau amarelo.\" -- Sinônimo<br>\n",
    "Exemplo3: \"tabua de cedrelinga cateniformis.\" -- Nome Ciêntifico<br>\n",
    "Além de problemas intrísecos ao Dataset, como as descrições de outros produtos que não são madeira com o NCM na nota fiscal como produto de madeira. Esses foram as maiores dificuldades que impactaram a qualidade da solução utilizanção LLM.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Essa atividade se mostrou uma boa oportunidade para revisitar esse problema.<br>\n",
    "Agora que tenho mais conhecimento na área de NLP, como eu posso solucionar essa tarefa?<br>\n",
    "Para isso eu vou partir do ponto em que eu reduzi o meu Dataset não rotulado para manter apenas as descrições únicas.<br>\n",
    "Essas descrições já passaram por um processo de pré-processamento simples, como: tornar tudo minúsculo e substituir acentos pelas letras normais.<br>\n",
    "Esse conjunto inicial é \"madeiraDescricao.csv\" e eu optei por manter apenas a coluna textual para evitar compartilhar dados sensíveis da nota.<br>\n",
    "Como os meus dados não estão rotulados essa é uma ótima oportunidade para utilizar o que aprendi com Weak Learning.<br>\n",
    "Primeiro preciso criar um conjunto de dados rotulados que será dividido em validação e teste. Mas para criar esse conjunto eu primeiro preciso entender como resolverei esses problemas.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "A solução que decidi seguir foi a implementação de uma tarefa de NER, em que as entidades serão as <mark>ESPECIE</mark>, <mark>CORTE</mark> E <mark>DIMENSAO</mark>.<br>\n",
    "Eu escolhi selecionar essa solução devido aos problemas mencionados, as multiplicidade das entidades, ou falta de alguma delas não seria um problema, além disso acredito que os problemas de sinônimos e nomes cientificos também são de fácil solução para essa tarefa.<br>\n",
    "Então para começar eu selecionei uma amostra aleatória de 600 instâncias e as removi do conjunto não rotulado, para fazer o rótulo dessas entidades eu optei por utilizar o Argilla devido a possibilidade de subir um container localmente, evitando o vazamento desses dados, uma etapa que seria necessária no meu ambiente do estágio.<br>\n",
    "O conjunto de dados que será rotulado é \"labedMadeiraDescricao.csv\" e o restante está no conjunto \"unlabeledMadeiraDescricao.csv\".<br>\n",
    "Esse foi o resultado final 564 dos 600 erão produtos de madeira e foram rotulados, o restante não será utilizado.<br>\n",
    "<img src=\"images/argilla.jpg\" alt=\"Argilla\" width=\"400\"><br>\n",
    "Agora que temos o conjunto de avaliação rotulado, \"labeledTokenClassification.csv\" podemos começar a resolver a tarefa.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "df_val_test = pd.read_csv('dataset/labeledTokenClassification.csv')\n",
    "df_unlabeled = pd.read_csv('dataset/unlabeledMadeiraDescricao.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Selecionar apenas as instâncias com o status de completo, para evitar utilizar as descrições que não são produtos de madeira.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_test = df_val_test[df_val_test.status == 'completed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Vamos separar em dois conjuntos, um para validação e outro para teste.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_val, df_test = train_test_split(\n",
    "    df_val_test,\n",
    "    test_size=0.5,       \n",
    "    random_state=1, # Necessário a mudança caso utilizar o conjunto de teste\n",
    "    shuffle=True         \n",
    ")\n",
    "print(len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21010"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversão para Objetos do Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skweak\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to artefatos/spacy_docs_train.bin...done\n",
      "Write to artefatos/spacy_docs_dev.bin...done\n",
      "Write to artefatos/spacy_docs_test.bin...done\n"
     ]
    }
   ],
   "source": [
    "spacy_docs_train = list(nlp.pipe(df_unlabeled[\"text\"].values))\n",
    "\n",
    "spacy_docs_dev = list(nlp.pipe(df_val[\"text\"].values))\n",
    "\n",
    "spacy_docs_test = list(nlp.pipe(df_test[\"text\"].values))\n",
    "\n",
    "skweak.utils.docbin_writer(spacy_docs_train, \"artefatos/spacy_docs_train.bin\")\n",
    "skweak.utils.docbin_writer(spacy_docs_dev, \"artefatos/spacy_docs_dev.bin\")\n",
    "skweak.utils.docbin_writer(spacy_docs_test, \"artefatos/spacy_docs_test.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training documents\n",
    "spacy_docs_train = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_train.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "\n",
    "# Load the validation documents\n",
    "spacy_docs_dev = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_dev.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "\n",
    "# Load the test documents\n",
    "spacy_docs_test = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_dev.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "\n",
    "# Convert the loaded documents to lists\n",
    "# This step ensures that the documents are in a list format, which is easier to work with in subsequent steps\n",
    "spacy_docs_train = list(spacy_docs_train)\n",
    "spacy_docs_dev = list(spacy_docs_dev)\n",
    "spacy_docs_test = list(spacy_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[angelim verm aproveitamento ripas, caibro mixto 4,5mt]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_docs_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Criação de funções de rotulagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Vamos precisar fazer funções de rotulagem para cada uma das entidades de interesse, <mark>ESPECIE</mark>, <mark>CORTE</mark>, <mark>DIMENSAO</mark>.<br>\n",
    "Para as funções de rotulagem de ESPECIE, podemos começar com o seu nome popular e sinônimos. Eu já tenho um dicionário que desenvolvi na época com as espécies mais relavantes para o nosso domínio no mercado do estado do RN.<br>\n",
    "Por isso irei partir dele e adicionar alguns casos simples para compensar a falta da função de Levenshtein que utilizei na época para aceitar pequenas alterações na variação das palavras.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "madeiras_dicionario = {\n",
    "    'acacia': ['angico'],\n",
    "    'acapu': ['angelim de folha larga'],\n",
    "    'amapa': [],\n",
    "    'amescla': ['amesclao', 'breu', 'morcegueira', 'sucuruba'],\n",
    "    'andiroba': ['angirova', 'carapa'],\n",
    "    'angelim amargoso': ['angelim amarelo', 'fava', 'fava amargosa', 'faveira'],\n",
    "    'angelim pedra': ['angelim verm', 'angelim vermelho', 'angelim falso', 'faveira ferro'],\n",
    "    'angelim indefinido': ['angelim', 'angelin', 'ang', 'ange'],\n",
    "    'cedrinho': ['quaruba', 'quaruvatinga', 'mandioqueira', 'quaruba vermelha'],\n",
    "    'cedro': [],\n",
    "    'cerejeira': [],\n",
    "    'cetim': ['pau amarelo', 'pau cetim', 'piquia cetim', 'garapa', ' muirajuba'],\n",
    "    'coco pau': ['baru'],\n",
    "    'copaiba': ['cupauba', 'copauva'],\n",
    "    'cumaru': ['cumaru de folha grande', 'cumbar', 'cumbaru roxo', 'cumbaru'],\n",
    "    'cupiuba': ['copiuba', 'copiuva'],\n",
    "    'eucalipto': ['euc', 'ecalipto', 'eucalipito'],\n",
    "    'freijo': [],\n",
    "    'garapeira': [],\n",
    "    'goiabao': ['abiu', 'abiurana'],\n",
    "    'guajara': ['currupixa', 'angico-bravo', 'angico-cedro', 'angico-rosa', 'curupia'],\n",
    "    'ipe': [],\n",
    "    'itauba': [],\n",
    "    'jatoba': ['jatai', 'utai', 'jati'],\n",
    "    'jequitiba': ['cachimbeiro', 'estopeira', 'coatinga', 'bingueiro'],\n",
    "    'louro amarelo': ['louro pardo, louro abacate', 'louro inhamui'],\n",
    "    'louro canela': ['louro rosa', 'louro gamela', 'louro vermelho'],\n",
    "    'louro indefinido': ['louro'],\n",
    "    'louro preto': [],\n",
    "    'marupa': [],\n",
    "    'massaranduba': ['macaranduba', 'masaranduba', 'maparajuba', 'aparaui', 'paraju', 'mass', 'massa'],\n",
    "    'matamata': [],\n",
    "    'melanceira': ['sucupira-pepino'],\n",
    "    'mogno': [],\n",
    "    'muiracatiara': [],\n",
    "    'para para': ['morototo', 'mandioqueira'],\n",
    "    'pau louro': [],\n",
    "    'pau mulato': [],\n",
    "    'peroba': ['amapa amargoso', 'parahancornia'],\n",
    "    'pinho': ['araucaria'],\n",
    "    'pinus': ['pinheiro', 'pinnus'],\n",
    "    'piquia': ['piqui', 'piquiarana', 'pequia'],\n",
    "    'roxinho': ['pau roxo', 'guarabu', 'pau ferro'],\n",
    "    'sapucaia': ['jarana'],\n",
    "    'sucupira': ['cutiba', 'sapupira', 'sebepira'],\n",
    "    'sumauma': ['ceiba'],\n",
    "    'tanimbuca': ['amarelao', 'mirindiba', 'embiridiba', 'cuiarana ,carana', 'mangue'],\n",
    "    'tatajuba': ['amarelo', 'amarelinho', 'garrote', 'bagaceira'],\n",
    "    'tauari': ['imbirema', 'estopeiro', 'toari'],\n",
    "    'taxi': [],\n",
    "    'timborana': ['timbauba', 'angico'],\n",
    "    'uxi': ['axua', 'cumate', 'paruru'],\n",
    "    'mista': ['misto', 'mixta', 'mix'],\n",
    "    'virola': ['ucuuba']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Além dos nomes populares e sinônimos também podemos fazer funções de rotulagem para a entidade ESPECIE utilizando o nome ciêntifico da espécie de madeira.<br>\n",
    "Essas funções não serão tão valiosas pela característica das descrições. Normalmente quando o nome ciêntifico está na descrição do produto ele está acompanhado do nome popular.<br>\n",
    "Mesmo assim eu achei relevante coletar essas entidades, porém irei utilizar um outro dicionário para tal, dessa forma poderemos diferenciar qual a fonte de rotulagem para a ESPECIE.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cientifico_madeiras_dicionario = {\n",
    "    'acapu': ['vouacapoua americana', 'vouacapoua'],\n",
    "    'amapa': ['parahancornia amapa', 'parahancornia'],\n",
    "    'amescla': ['protium heptaphyllum', 'protium', 'heptaphyllum'],\n",
    "    'andiroba': ['carapa guianensis', 'carapa', 'guianensis'],\n",
    "    'angelim amargoso': ['vatairea sericea', 'vatairea', 'sericea'],\n",
    "    'angelim pedra': ['hymenolobium petraeum', 'hymenolobium', 'petraeum'],\n",
    "    'angelim indefinido': ['dinizia excelsa', 'dinizia', 'excelsa'],\n",
    "    'cedrinho': ['erisma uncinatum', 'erisma', 'uncinatum'],\n",
    "    'cedro': ['cedrela fissilis', 'cedrela', 'fissilis'],\n",
    "    'cerejeira': ['amburana cearensis', 'amburana', 'cearensis'],\n",
    "    'cetim': ['cedrelinga cateniformis', 'cedrelinga', 'cateniformis'],\n",
    "    'coco pau': ['orbygnia phalerata', 'orbygnia', 'phalerata'],\n",
    "    'copaiba': ['copaifera langsdorffii', 'copaifera', 'langsdorffii'],\n",
    "    'cumaru': ['dipteryx odorata', 'dipteryx', 'odorata'],\n",
    "    'cupiuba': ['goupia glabra', 'goupia', 'glabra'],\n",
    "    'eucalipto': ['eucalyptus'],\n",
    "    'freijo': ['cordia goeldiana', 'cordia', 'goeldiana'],\n",
    "    'garapeira': ['apuleia leiocarpa', 'apuleia', 'leiocarpa'],\n",
    "    'goiabao': ['qualea'],\n",
    "    'guajara': ['terminalia amazonia', 'terminalia', 'amazonia'],\n",
    "    'ipe': ['handroanthus'],\n",
    "    'itauba': ['mezilaurus itauba', 'mezilaurus', 'itauba'],\n",
    "    'jatoba': ['hymenaea courbaril', 'hymenaea', 'courbaril'],\n",
    "    'jequitiba': ['cariniana legalis', 'cariniana', 'legalis'],\n",
    "    'louro amarelo': ['ocotea'],\n",
    "    'louro canela': ['nectandra'],\n",
    "    'louro indefinido': ['ocotea'],\n",
    "    'louro preto': ['ocotea porosa', 'ocotea', 'porosa'],\n",
    "    'marupa': ['simarouba amara', 'simarouba', 'amara'],\n",
    "    'massaranduba': ['manilkara huberi', 'manilkara', 'huberi'],\n",
    "    'matamata': ['eschweilera'],\n",
    "    'melanceira': ['trattinnickia burserifolia', 'trattinnickia', 'burserifolia'],\n",
    "    'mogno': ['swietenia macrophylla', 'swietenia', 'macrophylla'],\n",
    "    'muiracatiara': ['astronium lecointei', 'astronium', 'lecointei'],\n",
    "    'para para': ['jacaranda copaia', 'jacaranda', 'copaia'],\n",
    "    'pau louro': ['ocotea'],\n",
    "    'pau mulato': ['calycophyllum spruceanum', 'calycophyllum', 'spruceanum'],\n",
    "    'peroba': ['aspidosperma'],\n",
    "    'pinho': ['araucaria angustifolia', 'araucaria', 'angustifolia'],\n",
    "    'pinus': ['pinus'],\n",
    "    'piquia': ['caryocar villosum', 'caryocar', 'villosum'],\n",
    "    'roxinho': ['peltogyne'],\n",
    "    'sapucaia': ['lecythis pisonis', 'lecythis', 'pisonis'],\n",
    "    'sucupira': ['bowdichia virgilioides', 'bowdichia', 'virgilioides'],\n",
    "    'sumauma': ['ceiba pentandra', 'ceiba', 'pentandra'],\n",
    "    'tanimbuca': ['buchenavia tetraphylla', 'buchenavia', 'tetraphylla'],\n",
    "    'tatajuba': ['bagassa guianensis', 'bagassa', 'guianensis'],\n",
    "    'tauari': ['couratari'],\n",
    "    'taxi': ['sclerolobium'],\n",
    "    'timborana': ['enterolobium schomburgkii', 'enterolobium', 'schomburgkii'],\n",
    "    'uxi': ['endopleura uchi', 'endopleura', 'uchi'],\n",
    "    'virola': ['virola']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Para a entidade de <mark>CORTE</mark> podemos fazer um processo bem similar ao do primeiro dicionário de madeira, utilizando o nome popular do tipo de corte e o seu sinônimo.<br>\n",
    "Similar ao caso de dicionário também teremos que compensar pelo fator de que eu não estou usando Levenshtein, por isso precisamos aceitar erros comuns\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apresentacoes_dicionario ={\n",
    "                            'alizar': ['alisar'],\n",
    "                            'barrote':[],\n",
    "                            'caibro':['caibrinho'],\n",
    "                            'frechal': [],\n",
    "                            'janela': ['janelao'],\n",
    "                            'linha':[],\n",
    "                            'mourao': [],\n",
    "                            'quadrado': ['quadradro', 'bloco', 'file', 'quadradinho'],\n",
    "                            'piquete': [],\n",
    "                            'pontalete': [],\n",
    "                            'porta': ['portal', 'aduela'],\n",
    "                            'prancha':['pranchoes', 'pranchinha', 'pranchao'],\n",
    "                            'ripa':['ripao'],\n",
    "                            'tabua':['tauba'],\n",
    "                            'taipa':[],\n",
    "                            'sarrafo':['serrafo'],\n",
    "                            'viga':['virga','vigota', 'poste']\n",
    "                            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "E também irei complementar com o plural dos tipos de corte que terminam em vogal.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adc_plural(apresentacoes_dict):\n",
    "    updated_dict = {}\n",
    "    for key, synonyms in apresentacoes_dict.items():\n",
    "        pluralized_synonyms = []\n",
    "        \n",
    "        if key[-1].lower() in{'a', 'e', 'i', 'o', 'u'}:\n",
    "            plural = key + \"s\"\n",
    "        \n",
    "            if plural not in synonyms:\n",
    "                pluralized_synonyms.append(plural)\n",
    "\n",
    "        for term in synonyms:\n",
    "            pluralized_synonyms.append(term)\n",
    "            \n",
    "            if term and term[-1].lower() in {'a', 'e', 'i', 'o', 'u'}:\n",
    "                if \" \" in term:\n",
    "                    parts = term.split()\n",
    "                    parts[-1] += \"s\"\n",
    "                    plural = \" \".join(parts)\n",
    "                else:\n",
    "                    plural = term + \"s\"\n",
    "                pluralized_synonyms.append(plural)\n",
    "        \n",
    "        seen = set()\n",
    "        unique_synonyms = [t for t in pluralized_synonyms \n",
    "                          if not (t in seen or seen.add(t))]\n",
    "        \n",
    "        updated_dict[key] = unique_synonyms\n",
    "        \n",
    "    return updated_dict\n",
    "\n",
    "apresentacoes_dicionario = adc_plural(apresentacoes_dicionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "A coleta das dimensões do produto de madeira foi um dos maiores desafios, porque as descrições normalmente não seguem um padrão, o contribuinte quando escreve essa informação muitas vezes não coloca a dimensão associada a um número, como por exemplo \"linha massaranduba 5 x 12\", conseguimos assumir logicamente ao olhar para esse exemplo que não será 5 por 12 metros e muito provavelmente essa medida está em centímetros, existem alguns exemplos em que essa dimensão é ambígua, mas no geral conseguimos inferir.<br>\n",
    "Além disso, outro problema é que esses números podem estar soltos no texto, como em descrições do tipo \"linha 5x28 cm angelim c/ 6,0 mt\", então precisaremos considerar diversos tipos de padrões ao fazer as funções anotadoras para a entidade de <mark>DIMENSAO</mark>.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "## (PALAVRA) pode conter uma palavra | PALAVRA precisa conter uma palavra.\n",
    "padroes = [\n",
    "    # NUM X NUM X NUM (PALAVRA) -- 2.5 x 3 x 4 cm\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]*)', \n",
    "    # NUM PALAVRA X NUM PALAVRA X NUM PALAVRA -- 2.5 cm x 3 m x 4 mm\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\b', \n",
    "    # NUM X NUM (PALAVRA) -- 2 x 3 cm\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]*)', \n",
    "    # NUM PALAVRA X NUM PALAVRA -- 2cm x 3m\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\b',\n",
    "    # NUM X NUM -- 2.5 x 3\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\b',\n",
    "    # NUM (m, cm, mm, metro, pol) -- Mais restrito para evitar coletar número de série ou lixo nas descrições\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*(m|cm|mm|metro|metros|pol)\\b'\n",
    "]\n",
    "\n",
    "padroes_compilados = [re.compile(padrao, re.IGNORECASE) for padrao in padroes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Vamos criar uma heurística para a dimensão, se um dos padrões acontece inferimos que aquele termo está representando uma dimensão.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_dimensao(doc):\n",
    "    text = doc.text\n",
    "    for padrao in padroes_compilados:\n",
    "        for match in padrao.finditer(text):\n",
    "            start_char, end_char = match.span()\n",
    "            span = doc.char_span(start_char, end_char, alignment_mode=\"expand\")\n",
    "            if span:\n",
    "                yield span.start, span.end, \"DIMENSAO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skweak import heuristics\n",
    "dimensao_annotator = heuristics.FunctionAnnotator(\n",
    "    \"dimensao_annotator\", \n",
    "    detectar_dimensao,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skweak.gazetteers import GazetteerAnnotator, Trie\n",
    "from skweak.base import CombinedAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "especie_terms = []\n",
    "for key, synonyms in madeiras_dicionario.items():\n",
    "    especie_terms.append(key)\n",
    "    especie_terms.extend(synonyms)\n",
    "\n",
    "cientifico_especie_terms = []\n",
    "for key, cientifico in cientifico_madeiras_dicionario.items():\n",
    "    cientifico_especie_terms.extend(cientifico)\n",
    "\n",
    "corte_terms = []\n",
    "for key, synonyms in apresentacoes_dicionario.items():\n",
    "    corte_terms.append(key)\n",
    "    corte_terms.extend(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trie(term_list):\n",
    "    \"\"\"Convert a list of terms into a trie of token sequences\"\"\"\n",
    "    trie = Trie()\n",
    "    for term in term_list:\n",
    "        tokens = term.split()\n",
    "        trie.add(tokens)\n",
    "    return trie\n",
    "\n",
    "especie_trie = build_trie(especie_terms)\n",
    "cientifico_especie_trie = build_trie(cientifico_especie_terms)\n",
    "corte_trie = build_trie(corte_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['vouacapoua'], ['vouacapoua', 'americana'], ['parahancornia'], ['parahancornia', 'amapa'], ['protium'], ['protium', 'heptaphyllum'], ['heptaphyllum'], ['carapa'], ['carapa', 'guianensis'], ['guianensis'], ['vatairea'], ['vatairea', 'sericea'], ['sericea'], ['hymenolobium'], ['hymenolobium', 'petraeum'], ['petraeum'], ['dinizia'], ['dinizia', 'excelsa'], ['excelsa'], ['erisma'], ['erisma', 'uncinatum'], ['uncinatum'], ['cedrela'], ['cedrela', 'fissilis'], ['fissilis'], ['amburana'], ['amburana', 'cearensis'], ['cearensis'], ['cedrelinga'], ['cedrelinga', 'cateniformis'], ['cateniformis'], ['orbygnia'], ['orbygnia', 'phalerata'], ['phalerata'], ['copaifera'], ['copaifera', 'langsdorffii'], ['langsdorffii'], ['dipteryx'], ['dipteryx', 'odorata'], ['odorata'], ['goupia'], ['goupia', 'glabra'], ['glabra'], ['eucalyptus'], ['cordia'], ['cordia', 'goeldiana'], ['goeldiana'], ['apuleia'], ['apuleia', 'leiocarpa'], ['leiocarpa'], ['qualea'], ['terminalia'], ['terminalia', 'amazonia'], ['amazonia'], ['handroanthus'], ['mezilaurus'], ['mezilaurus', 'itauba'], ['itauba'], ['hymenaea'], ['hymenaea', 'courbaril'], ['courbaril'], ['cariniana'], ['cariniana', 'legalis'], ['legalis'], ['ocotea'], ['ocotea', 'porosa'], ['nectandra'], ['porosa'], ['simarouba'], ['simarouba', 'amara'], ['amara'], ['manilkara'], ['manilkara', 'huberi'], ['huberi'], ['eschweilera'], ['trattinnickia'], ['trattinnickia', 'burserifolia'], ['burserifolia'], ['swietenia'], ['swietenia', 'macrophylla'], ['macrophylla'], ['astronium'], ['astronium', 'lecointei'], ['lecointei'], ['jacaranda'], ['jacaranda', 'copaia'], ['copaia'], ['calycophyllum'], ['calycophyllum', 'spruceanum'], ['spruceanum'], ['aspidosperma'], ['araucaria'], ['araucaria', 'angustifolia'], ['angustifolia'], ['pinus'], ['caryocar'], ['caryocar', 'villosum'], ['villosum'], ['peltogyne'], ['lecythis'], ['lecythis', 'pisonis'], ['pisonis'], ['bowdichia'], ['bowdichia', 'virgilioides'], ['virgilioides'], ['ceiba'], ['ceiba', 'pentandra'], ['pentandra'], ['buchenavia'], ['buchenavia', 'tetraphylla'], ['tetraphylla'], ['bagassa'], ['bagassa', 'guianensis'], ['couratari'], ['sclerolobium'], ['enterolobium'], ['enterolobium', 'schomburgkii'], ['schomburgkii'], ['endopleura'], ['endopleura', 'uchi'], ['uchi'], ['virola']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cientifico_especie_trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "especie_annotator = GazetteerAnnotator(\"especie_annotator\", {\"ESPECIE\": especie_trie})\n",
    "cientifico_especie_annotator = GazetteerAnnotator(\"cientifico_especie_annotator\", {\"ESPECIE\": cientifico_especie_trie})\n",
    "corte_annotator = GazetteerAnnotator(\"corte_annotator\", {\"CORTE\": corte_trie})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<skweak.base.CombinedAnnotator at 0x16f584293c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = CombinedAnnotator()\n",
    "combined.add_annotator(especie_annotator)\n",
    "combined.add_annotator(cientifico_especie_annotator)\n",
    "combined.add_annotator(corte_annotator)\n",
    "combined.add_annotator(dimensao_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">madeira serrada \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    massaranduba\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    manilkara huberi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       ") \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2.5 metro\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DIMENSAO</span>\n",
       "</mark>\n",
       " 2 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    vigas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       " de \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    10x15cm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DIMENSAO</span>\n",
       "</mark>\n",
       " e 3 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tabuas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       " de \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2,5x30x200cm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DIMENSAO</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"madeira serrada massaranduba (manilkara huberi) 2.5 metro 2 vigas de 10x15cm e 3 tabuas de 2,5x30x200cm\")\n",
    "combined(doc)\n",
    "skweak.utils.display_entities(doc, [\"especie_annotator\", \"cientifico_especie_annotator\", \"corte_annotator\", \"dimensao_annotator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESPECIE: ['massaranduba']\n",
      "ESPECIE: ['manilkara huberi']\n",
      "CORTE: ['vigas', 'tabuas']\n",
      "DIMENSAO: ['2,5x30x200cm', '10x15cm', '2,5x30x200', '2.5 metro']\n"
     ]
    }
   ],
   "source": [
    "print(\"ESPECIE:\", [ent.text for ent in doc.spans[\"especie_annotator\"]])\n",
    "print(\"ESPECIE:\", [ent.text for ent in doc.spans[\"cientifico_especie_annotator\"]])\n",
    "print(\"CORTE:\", [ent.text for ent in doc.spans[\"corte_annotator\"]])\n",
    "print(\"DIMENSAO:\", [ent.text for ent in doc.spans[\"dimensao_annotator\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Utilizando um exemplo do conjunto de treinamento.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    angelim verm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " aproveitamento \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ripas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = spacy_docs_train[0]\n",
    "\n",
    "combined(doc)\n",
    "\n",
    "skweak.utils.display_entities(doc, [\"especie_annotator\", \"cientifico_especie_annotator\", \"corte_annotator\", \"dimensao_annotator\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Gliner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Além dos anotadores que criamos com a lista de espécies e corte de madeira, e por o problema ser bem específico não fui capaz de achar nenhum tipo de modelo pré-treinado para a tarefa de NER do domínio selecionado.<br>\n",
    "Por isso vamos utilizar os modelos zero shot do GliNER, utilizando os nossos domínios esses modelos irão tentar rotular o texto sem nenhum treinamento prévio, o que pode ajudar bastante a solução da tarefa.  \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [02:15<00:00, 12.27s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Fetching 10 files: 100%|██████████| 10/10 [00:53<00:00,  5.36s/it]\n",
      "Fetching 12 files: 100%|██████████| 12/12 [02:48<00:00, 14.05s/it]\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\gliner\\model.py:91: UserWarning: Vocab size of the model (151648) does't match length of tokenizer (151649). \n",
      "                            You should to consider manually add new tokens to tokenizer or to load tokenizer with added tokens.\n",
      "  warnings.warn(f\"\"\"Vocab size of the model ({config.vocab_size}) does't match length of tokenizer ({len(self.data_processor.transformer_tokenizer)}).\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model_gliner_llama = GLiNER.from_pretrained(\n",
    "    \"knowledgator/gliner-llama-1.3B-v1.0\",\n",
    ")\n",
    "\n",
    "model_gliner_bi_large = GLiNER.from_pretrained(\n",
    "    \"knowledgator/gliner-bi-large-v1.0\",\n",
    ")\n",
    "\n",
    "model_gliner_qwen = GLiNER.from_pretrained(\n",
    "    \"knowledgator/gliner-qwen-1.5B-v1.0\",\n",
    ")\n",
    "\n",
    "model_gliner_llama = model_gliner_llama.half()\n",
    "model_gliner_bi_large = model_gliner_bi_large.half()\n",
    "model_gliner_qwen = model_gliner_qwen.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"madeira serrada massaranduba (manilkara huberi) 2.5 metro 2 vigas de 10x15cm e 3 tabuas de 2,5x30x200cm\")\n",
    "text = doc.text\n",
    "\n",
    "labels = [\"ESPECIE\", \"CORTE\", \"DIMENSAO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "from typing import Any\n",
    "\n",
    "def render_entity_data_from_pipeline(\n",
    "    text: str,\n",
    "    pipeline_results: list[dict[str, Any]],\n",
    "    colors: dict[str, str] = None,\n",
    "    label_key_name: str = \"entity_group\",\n",
    ") -> None:\n",
    "    # Extract entity spans (start, end, label) from pipeline results\n",
    "    entity_spans = [\n",
    "        (result[\"start\"], result[\"end\"], result[label_key_name])\n",
    "        for result in pipeline_results\n",
    "    ]\n",
    "\n",
    "    # If no colors are provided, assign default colors to each entity type\n",
    "    if colors is None:\n",
    "        entity_types = list(set(span[2] for span in entity_spans))\n",
    "        # Default colors for up to 5 entity types; cycle if more\n",
    "        default_colors = ['#FF0000', '#00FF00', '#0000FF', '#FFFF00', '#FF00FF']\n",
    "        colors = {}\n",
    "        for i, entity_type in enumerate(entity_types):\n",
    "            colors[entity_type] = default_colors[i % len(default_colors)]\n",
    "\n",
    "    # Configure displacy options with entity labels and their colors\n",
    "    displacy_options = {\n",
    "        \"ents\": list(colors.keys()),\n",
    "        \"colors\": colors  # Pass the color mapping dictionary\n",
    "    }\n",
    "\n",
    "    # Prepare data in the format required by displacy\n",
    "    displacy_data = [{\n",
    "        \"text\": text,\n",
    "        \"ents\": [\n",
    "            {\"start\": start, \"end\": end, \"label\": label}\n",
    "            for start, end, label in entity_spans\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    # Render the entities using displacy\n",
    "    displacy.render(\n",
    "        displacy_data,\n",
    "        style=\"ent\",\n",
    "        manual=True,\n",
    "        jupyter=True,\n",
    "        options=displacy_options\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">madeira serrada massaranduba (manilkara huberi) 2.5 metro 2 vigas de 10x15cm e 3 tabuas de 2,5x30x200cm</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = model_gliner_llama.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "print(result)\n",
    "\n",
    "render_entity_data_from_pipeline(text, result, label_key_name=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">madeira serrada massaranduba (manilkara huberi) 2.5 metro 2 vigas de 10x15cm e 3 tabuas de 2,5x30x200cm</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = model_gliner_bi_large.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "print(result)\n",
    "\n",
    "render_entity_data_from_pipeline(text, result, label_key_name=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0, 'end': 28, 'text': 'madeira serrada massaranduba', 'label': 'ESPECIE', 'score': 0.9453125}, {'start': 30, 'end': 46, 'text': 'manilkara huberi', 'label': 'ESPECIE', 'score': 0.8544921875}, {'start': 48, 'end': 57, 'text': '2.5 metro', 'label': 'DIMENSAO', 'score': 0.97607421875}, {'start': 58, 'end': 59, 'text': '2', 'label': 'DIMENSAO', 'score': 0.5771484375}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #FF0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    madeira serrada massaranduba\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #FF0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    manilkara huberi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       ") \n",
       "<mark class=\"entity\" style=\"background: #00FF00; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2.5 metro\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DIMENSAO</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #00FF00; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DIMENSAO</span>\n",
       "</mark>\n",
       " vigas de 10x15cm e 3 tabuas de 2,5x30x200cm</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = model_gliner_qwen.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "print(result)\n",
    "\n",
    "render_entity_data_from_pipeline(text, result, label_key_name=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Okay não começamos bem. Para o nosso exemplo simples apenas o modelo do Qwen foi capaz de identicar algumas entidades, parece que a entidade de corte foi um desafio para esses modelos.<br>\n",
    "Vamos observar em uma quantidade maior de dados, vamos utilizar 50 instâncias do conjunto de treinamento para ver se podemos utilizar esses modelos como anotadores para o nosso problema.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_test = [doc.text for doc in spacy_docs_train[:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def create_entity_dataframe(texts, model, labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Process multiple texts and create a DataFrame with entities\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts to process\n",
    "        model: Your GLiNER model\n",
    "        labels (list): Entity labels to predict\n",
    "        threshold (float): Confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with text, entities, and entity count\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for text in tqdm(texts):  # tqdm adds progress bar\n",
    "        # Get predictions from model\n",
    "        entities = model.predict_entities(text, labels, threshold=threshold)\n",
    "        \n",
    "        # Format results\n",
    "        data.append({\n",
    "            \"text\": text,\n",
    "            \"entities\": entities,\n",
    "            \"n_entities\": len(entities)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:34<00:00,  3.09s/it]\n"
     ]
    }
   ],
   "source": [
    "llama_df = create_entity_dataframe(\n",
    "    texts=texts_to_test,\n",
    "    model=model_gliner_llama,\n",
    "    labels=labels,\n",
    "    threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Contando a quantidade de entidades que o Llama foi capaz de reconhecer.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_df.n_entities.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "O llama não foi capaz de achar entidades na amostra de 50 instâncias do conjunto de dados não rotulados, portanto já podemos eliminar o uso desse modelo da nossa pipeline.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:00<00:00,  7.21s/it]\n"
     ]
    }
   ],
   "source": [
    "bi_large_df = create_entity_dataframe(\n",
    "    texts=texts_to_test,\n",
    "    model=model_gliner_bi_large,\n",
    "    labels=labels,\n",
    "    threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_large_df.n_entities.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ripa</td>\n",
       "      <td>[{'start': 0, 'end': 4, 'text': 'ripa', 'label': 'DIMENSAO', 'score': 0.51904296875}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>caibros</td>\n",
       "      <td>[{'start': 0, 'end': 7, 'text': 'caibros', 'label': 'DIMENSAO', 'score': 0.52880859375}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>linha 5x24 angelim 4.50mt</td>\n",
       "      <td>[{'start': 11, 'end': 18, 'text': 'angelim', 'label': 'DIMENSAO', 'score': 0.50830078125}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         text                                                                                    entities\n",
       "13                       ripa       [{'start': 0, 'end': 4, 'text': 'ripa', 'label': 'DIMENSAO', 'score': 0.51904296875}]\n",
       "31                    caibros    [{'start': 0, 'end': 7, 'text': 'caibros', 'label': 'DIMENSAO', 'score': 0.52880859375}]\n",
       "41  linha 5x24 angelim 4.50mt  [{'start': 11, 'end': 18, 'text': 'angelim', 'label': 'DIMENSAO', 'score': 0.50830078125}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_large_entities = bi_large_df[bi_large_df.n_entities > 0]\n",
    "bi_large_entities[['text', 'entities']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "O modelo bi_large foi capaz de achar 3 entidades, porém todas com a entidade errada, também podemos remover esse modelo da pipeline.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:19<00:00,  6.38s/it]\n"
     ]
    }
   ],
   "source": [
    "qwen_df = create_entity_dataframe(\n",
    "    texts=texts_to_test,\n",
    "    model=model_gliner_qwen,\n",
    "    labels=labels,\n",
    "    threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(23)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_df.n_entities.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angelim verm aproveitamento ripas</td>\n",
       "      <td>[{'start': 0, 'end': 12, 'text': 'angelim verm', 'label': 'ESPECIE', 'score': 0.6279296875}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caibro 5x3 angelin 5.50mt</td>\n",
       "      <td>[{'start': 0, 'end': 10, 'text': 'caibro 5x3', 'label': 'ESPECIE', 'score': 0.7294921875}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tabua de pinnus aplainada 3m x 15cm</td>\n",
       "      <td>[{'start': 0, 'end': 15, 'text': 'tabua de pinnus', 'label': 'ESPECIE', 'score': 0.837890625}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>madeirite 10mm 2,20x1,10mm resinado</td>\n",
       "      <td>[{'start': 0, 'end': 14, 'text': 'madeirite 10mm', 'label': 'ESPECIE', 'score': 0.74169921875}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>frechal 5x4 angelim 4.50mt</td>\n",
       "      <td>[{'start': 0, 'end': 11, 'text': 'frechal 5x4', 'label': 'ESPECIE', 'score': 0.759765625}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>frechal 5x4 angelim 7.00mt</td>\n",
       "      <td>[{'start': 0, 'end': 11, 'text': 'frechal 5x4', 'label': 'ESPECIE', 'score': 0.74365234375}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>porta laminada jatoba 210 x 90 pormatex</td>\n",
       "      <td>[{'start': 0, 'end': 21, 'text': 'porta laminada jatoba', 'label': 'ESPECIE', 'score': 0.8032226...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>madeira serrada viga de macaranduba</td>\n",
       "      <td>[{'start': 0, 'end': 35, 'text': 'madeira serrada viga de macaranduba', 'label': 'CORTE', 'score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>caibro massa 2,5mt</td>\n",
       "      <td>[{'start': 0, 'end': 14, 'text': 'caibro massa 2', 'label': 'ESPECIE', 'score': 0.88525390625}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>guajara bolacha mad ind de residuo (short-sarrafo)</td>\n",
       "      <td>[{'start': 0, 'end': 34, 'text': 'guajara bolacha mad ind de residuo', 'label': 'ESPECIE', 'scor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sarrafo 7x2cm pinus</td>\n",
       "      <td>[{'start': 0, 'end': 13, 'text': 'sarrafo 7x2cm', 'label': 'ESPECIE', 'score': 0.63818359375}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>muiracatiara serrada</td>\n",
       "      <td>[{'start': 0, 'end': 12, 'text': 'muiracatiara', 'label': 'CORTE', 'score': 0.59375}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>macaranduba madeira serrada ripa</td>\n",
       "      <td>[{'start': 0, 'end': 11, 'text': 'macaranduba', 'label': 'ESPECIE', 'score': 0.7197265625}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>caibro macaranduba</td>\n",
       "      <td>[{'start': 0, 'end': 18, 'text': 'caibro macaranduba', 'label': 'ESPECIE', 'score': 0.6533203125}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ripa mista 1,3x5x1m</td>\n",
       "      <td>[{'start': 0, 'end': 10, 'text': 'ripa mista', 'label': 'ESPECIE', 'score': 0.7314453125}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>linhas de 3,5 metros de 14cm</td>\n",
       "      <td>[{'start': 0, 'end': 20, 'text': 'linhas de 3,5 metros', 'label': 'DIMENSAO', 'score': 0.5532226...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>madeira angelim vermelho serrada em vigota</td>\n",
       "      <td>[{'start': 0, 'end': 24, 'text': 'madeira angelim vermelho', 'label': 'ESPECIE', 'score': 0.8007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>jatoba prancha - m</td>\n",
       "      <td>[{'start': 0, 'end': 14, 'text': 'jatoba prancha', 'label': 'ESPECIE', 'score': 0.95068359375}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>jatoba serd pranchas</td>\n",
       "      <td>[{'start': 0, 'end': 20, 'text': 'jatoba serd pranchas', 'label': 'ESPECIE', 'score': 0.60693359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>macaranduba serr. ind.res.ripa mani</td>\n",
       "      <td>[{'start': 0, 'end': 35, 'text': 'macaranduba serr. ind.res.ripa mani', 'label': 'ESPECIE', 'sco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text                                                                                             entities\n",
       "0                    angelim verm aproveitamento ripas         [{'start': 0, 'end': 12, 'text': 'angelim verm', 'label': 'ESPECIE', 'score': 0.6279296875}]\n",
       "2                            caibro 5x3 angelin 5.50mt           [{'start': 0, 'end': 10, 'text': 'caibro 5x3', 'label': 'ESPECIE', 'score': 0.7294921875}]\n",
       "3                  tabua de pinnus aplainada 3m x 15cm       [{'start': 0, 'end': 15, 'text': 'tabua de pinnus', 'label': 'ESPECIE', 'score': 0.837890625}]\n",
       "5                  madeirite 10mm 2,20x1,10mm resinado  [{'start': 0, 'end': 14, 'text': 'madeirite 10mm', 'label': 'ESPECIE', 'score': 0.74169921875}, ...\n",
       "7                           frechal 5x4 angelim 4.50mt           [{'start': 0, 'end': 11, 'text': 'frechal 5x4', 'label': 'ESPECIE', 'score': 0.759765625}]\n",
       "10                          frechal 5x4 angelim 7.00mt         [{'start': 0, 'end': 11, 'text': 'frechal 5x4', 'label': 'ESPECIE', 'score': 0.74365234375}]\n",
       "11             porta laminada jatoba 210 x 90 pormatex  [{'start': 0, 'end': 21, 'text': 'porta laminada jatoba', 'label': 'ESPECIE', 'score': 0.8032226...\n",
       "12                 madeira serrada viga de macaranduba  [{'start': 0, 'end': 35, 'text': 'madeira serrada viga de macaranduba', 'label': 'CORTE', 'score...\n",
       "14                                  caibro massa 2,5mt      [{'start': 0, 'end': 14, 'text': 'caibro massa 2', 'label': 'ESPECIE', 'score': 0.88525390625}]\n",
       "15  guajara bolacha mad ind de residuo (short-sarrafo)  [{'start': 0, 'end': 34, 'text': 'guajara bolacha mad ind de residuo', 'label': 'ESPECIE', 'scor...\n",
       "16                                 sarrafo 7x2cm pinus       [{'start': 0, 'end': 13, 'text': 'sarrafo 7x2cm', 'label': 'ESPECIE', 'score': 0.63818359375}]\n",
       "17                                muiracatiara serrada                [{'start': 0, 'end': 12, 'text': 'muiracatiara', 'label': 'CORTE', 'score': 0.59375}]\n",
       "18                    macaranduba madeira serrada ripa          [{'start': 0, 'end': 11, 'text': 'macaranduba', 'label': 'ESPECIE', 'score': 0.7197265625}]\n",
       "19                                  caibro macaranduba   [{'start': 0, 'end': 18, 'text': 'caibro macaranduba', 'label': 'ESPECIE', 'score': 0.6533203125}]\n",
       "23                                 ripa mista 1,3x5x1m           [{'start': 0, 'end': 10, 'text': 'ripa mista', 'label': 'ESPECIE', 'score': 0.7314453125}]\n",
       "30                        linhas de 3,5 metros de 14cm  [{'start': 0, 'end': 20, 'text': 'linhas de 3,5 metros', 'label': 'DIMENSAO', 'score': 0.5532226...\n",
       "37          madeira angelim vermelho serrada em vigota  [{'start': 0, 'end': 24, 'text': 'madeira angelim vermelho', 'label': 'ESPECIE', 'score': 0.8007...\n",
       "40                                  jatoba prancha - m      [{'start': 0, 'end': 14, 'text': 'jatoba prancha', 'label': 'ESPECIE', 'score': 0.95068359375}]\n",
       "43                                jatoba serd pranchas  [{'start': 0, 'end': 20, 'text': 'jatoba serd pranchas', 'label': 'ESPECIE', 'score': 0.60693359...\n",
       "44                 macaranduba serr. ind.res.ripa mani  [{'start': 0, 'end': 35, 'text': 'macaranduba serr. ind.res.ripa mani', 'label': 'ESPECIE', 'sco..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_entities = qwen_df[qwen_df.n_entities > 0]\n",
    "qwen_entities[['text', 'entities']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "O qwen foi o melhor dos 3 modelos, além de ter achado a maior quantidade de entidades grande parte delas está correta, porém existem alguns erros preocupantes, o texto da linha 12 foi completamente selecionada, realmente existe um CORTE, na descrição mas ele não foi devidamente selecionado, além disso Muiracatiara na linha 17 foi selecionado como corte, mas deveria ser uma espécie.<br>\n",
    "Vamos observar os casos em que o modelo não selecionou nenhuma entidade para analisar se estamos tendo um bom coverage ao menos do rótulo desses textos.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caibro mixto 4,5mt</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trave 2x10cm</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mad. esp. ang. vermelho industrializada de residuo</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jogo cozinha em madeira</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>conjunto de utensilios para cozi</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ripa</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>linha 5x20cm mista</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>caibro 3x5 4.50mt de massaranduba</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>palitos p/ dentes aurea gde</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>linha 5x20cm massaranduba</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>massaranduba viga</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>espeto p/churrasco bambu fiat lux 10x100</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>caibros 5x3.5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>linha mix 3x6 5,5mt</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>linhas de 5,5 metros de 14c</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>caibros</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>linha 5x12cm mista</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>pv-kit colher / espatula em made</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tabua de 30x2.2cm</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>linha mass 5 x 24 - 6,50</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>madeirite plastificado 1,10 x 2,20 12mm</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>linha mass 5 x 24 - 3,00</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>porta jatoba diag 0,90 x 2,10</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>linha 5x24 angelim 4.50mt</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>linha mix 3x4 4,5mt</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tabua 30x3,0cm mista</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>linhas de 7,5 meros de 12 cm</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>linha 5 x 11.5cm 3,50mt ang.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>linha 5x14cm mista</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ripa angelim 5x1.0cm</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text entities\n",
       "1                                   caibro mixto 4,5mt       []\n",
       "4                                         trave 2x10cm       []\n",
       "6   mad. esp. ang. vermelho industrializada de residuo       []\n",
       "8                              jogo cozinha em madeira       []\n",
       "9                     conjunto de utensilios para cozi       []\n",
       "13                                                ripa       []\n",
       "20                                  linha 5x20cm mista       []\n",
       "21                   caibro 3x5 4.50mt de massaranduba       []\n",
       "22                         palitos p/ dentes aurea gde       []\n",
       "24                           linha 5x20cm massaranduba       []\n",
       "25                                   massaranduba viga       []\n",
       "26            espeto p/churrasco bambu fiat lux 10x100       []\n",
       "27                                       caibros 5x3.5       []\n",
       "28                                 linha mix 3x6 5,5mt       []\n",
       "29                         linhas de 5,5 metros de 14c       []\n",
       "31                                             caibros       []\n",
       "32                                  linha 5x12cm mista       []\n",
       "33                    pv-kit colher / espatula em made       []\n",
       "34                                   tabua de 30x2.2cm       []\n",
       "35                            linha mass 5 x 24 - 6,50       []\n",
       "36             madeirite plastificado 1,10 x 2,20 12mm       []\n",
       "38                            linha mass 5 x 24 - 3,00       []\n",
       "39                       porta jatoba diag 0,90 x 2,10       []\n",
       "41                           linha 5x24 angelim 4.50mt       []\n",
       "42                                 linha mix 3x4 4,5mt       []\n",
       "45                                tabua 30x3,0cm mista       []\n",
       "46                        linhas de 7,5 meros de 12 cm       []\n",
       "47                        linha 5 x 11.5cm 3,50mt ang.       []\n",
       "48                                  linha 5x14cm mista       []\n",
       "49                                ripa angelim 5x1.0cm       []"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_not_entities = qwen_df[qwen_df.n_entities == 0]\n",
    "qwen_not_entities[['text', 'entities']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "No conjunto de texto em que o Qwen não foi capaz de identificar nenhuma das entidades existem exemplos como a linha 8, 9, 22 que realmente não deveriam ser rotulados, porém a grande maioria deveria.<br>\n",
    "Por esse motivo e o grande custo que esses modelos possuem comparados as funções de rotulagem que criamos antes também não irei utilizar o Zero shot do Qwen como uma função rotuladora.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aplicando Funções de Agregação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_docs_train = list(combined.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Salvar o objeto do Spacy para não precisar rodar o processo anterior múltiplas vezes.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to artefatos/spacy_docs_train_annotated.bin...done\n"
     ]
    }
   ],
   "source": [
    "skweak.utils.docbin_writer(\n",
    "    spacy_docs_train, \"artefatos/spacy_docs_train_annotated.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skweak\n",
    "\n",
    "# Now we can load them back, so we don't need to run the labelling functions again\n",
    "spacy_docs_train = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_train_annotated.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "spacy_docs_train = list(spacy_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_voter = skweak.aggregation.MajorityVoter(\n",
    "    name=\"doclevel_voter\",\n",
    "    labels=[\"ESPECIE\", \"CORTE\", \"DIMENSAO\"],\n",
    "    initial_weights={\"doc_majority\": 0.0}  # Evitar a inclusão dele mesmo\n",
    ")\n",
    "\n",
    "spacy_docs_train = list(majority_voter.pipe(spacy_docs_train))\n",
    "\n",
    "doc_majority = skweak.doclevel.DocumentMajorityAnnotator(\"doc_majority\",\"doclevel_voter\")\n",
    "\n",
    "spacy_docs_train = list(doc_majority.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19902 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1 -132049.32920927             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19902 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2 -127019.76980639   +5029.55940288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19902 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3 -125608.88917045   +1410.88063594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19902 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4 -124916.46359757    +692.42557288\n"
     ]
    }
   ],
   "source": [
    "initial_weights = {\n",
    "    \"especie_annotator\": .9,\n",
    "    \"cientifico_especie_annotator\": 1,\n",
    "    \"corte_annotator\": 1,\n",
    "    \"dimensao_annotator\": 1,\n",
    "    \"doclevel_voter\": 0.5,\n",
    "    \"doc_majority\": 0.6\n",
    "}\n",
    "\n",
    "hmm = skweak.generative.HMM(\n",
    "    \"hmm\", \n",
    "    labels=[\"ESPECIE\", \"CORTE\", \"DIMENSAO\"],\n",
    "    initial_weights=initial_weights\n",
    ")\n",
    "\n",
    "\n",
    "hmm.fit(spacy_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM model with following parameters:\n",
      "Output labels: ['O', 'B-ESPECIE', 'I-ESPECIE', 'B-CORTE', 'I-CORTE', 'B-DIMENSAO', 'I-DIMENSAO']\n",
      "--------\n",
      "Start distribution:\n",
      "O             0.49\n",
      "B-ESPECIE     0.16\n",
      "I-ESPECIE     0.00\n",
      "B-CORTE       0.35\n",
      "I-CORTE       0.00\n",
      "B-DIMENSAO    0.00\n",
      "I-DIMENSAO    0.00\n",
      "dtype: float64\n",
      "--------\n",
      "Transition model:\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.78       0.03       0.00     0.14      0.0        0.05        0.00\n",
      "B-ESPECIE   0.65       0.00       0.16     0.00      0.0        0.19        0.00\n",
      "I-ESPECIE   0.86       0.00       0.00     0.00      0.0        0.14        0.00\n",
      "B-CORTE     0.66       0.22       0.00     0.01      0.0        0.12        0.00\n",
      "I-CORTE     0.20       0.20       0.00     0.20      0.2        0.20        0.00\n",
      "B-DIMENSAO  0.03       0.00       0.00     0.00      0.0        0.01        0.96\n",
      "I-DIMENSAO  0.41       0.03       0.00     0.00      0.0        0.17        0.39\n",
      "--------\n",
      "Labelling functions in model: ['dimensao_annotator', 'cientifico_especie_annotator', 'especie_annotator', 'doc_majority', 'corte_annotator']\n",
      "Emission model for: cientifico_especie_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.90       0.06       0.04     0.00     0.00        0.00        0.00\n",
      "B-ESPECIE   0.01       0.98       0.00     0.00     0.00        0.00        0.00\n",
      "I-ESPECIE   0.00       0.00       1.00     0.00     0.00        0.00        0.00\n",
      "B-CORTE     1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-CORTE     0.23       0.00       0.00     0.00     0.77        0.00        0.00\n",
      "B-DIMENSAO  1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-DIMENSAO  0.99       0.01       0.00     0.00     0.00        0.00        0.00\n",
      "weights     0.95       0.96       1.00     0.93     0.93        0.93        0.93\n",
      "--------\n",
      "Emission model for: corte_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.99        0.0        0.0     0.01     0.00         0.0         0.0\n",
      "B-ESPECIE   1.00        0.0        0.0     0.00     0.00         0.0         0.0\n",
      "I-ESPECIE   1.00        0.0        0.0     0.00     0.00         0.0         0.0\n",
      "B-CORTE     0.00        0.0        0.0     1.00     0.00         0.0         0.0\n",
      "I-CORTE     0.23        0.0        0.0     0.00     0.77         0.0         0.0\n",
      "B-DIMENSAO  1.00        0.0        0.0     0.00     0.00         0.0         0.0\n",
      "I-DIMENSAO  0.99        0.0        0.0     0.01     0.00         0.0         0.0\n",
      "weights     1.00        1.0        1.0     1.00     1.00         1.0         1.0\n",
      "--------\n",
      "Emission model for: dimensao_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.91        0.0        0.0      0.0     0.00        0.09         0.0\n",
      "B-ESPECIE   1.00        0.0        0.0      0.0     0.00        0.00         0.0\n",
      "I-ESPECIE   1.00        0.0        0.0      0.0     0.00        0.00         0.0\n",
      "B-CORTE     1.00        0.0        0.0      0.0     0.00        0.00         0.0\n",
      "I-CORTE     0.23        0.0        0.0      0.0     0.77        0.00         0.0\n",
      "B-DIMENSAO  0.00        0.0        0.0      0.0     0.00        1.00         0.0\n",
      "I-DIMENSAO  0.00        0.0        0.0      0.0     0.00        0.00         1.0\n",
      "weights     1.00        1.0        1.0      1.0     1.00        1.00         1.0\n",
      "--------\n",
      "Emission model for: doc_majority\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.94       0.06       0.00     0.00     0.00        0.00        0.00\n",
      "B-ESPECIE   0.20       0.80       0.00     0.00     0.00        0.00        0.00\n",
      "I-ESPECIE   0.05       0.59       0.36     0.00     0.00        0.00        0.00\n",
      "B-CORTE     0.52       0.00       0.00     0.48     0.00        0.00        0.00\n",
      "I-CORTE     0.23       0.00       0.00     0.00     0.77        0.00        0.00\n",
      "B-DIMENSAO  0.76       0.00       0.00     0.00     0.00        0.24        0.00\n",
      "I-DIMENSAO  0.71       0.14       0.00     0.00     0.00        0.00        0.15\n",
      "weights     0.60       0.60       0.60     0.60     0.60        0.60        0.60\n",
      "--------\n",
      "Emission model for: especie_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.90       0.10       0.00     0.00     0.00        0.00        0.00\n",
      "B-ESPECIE   0.00       1.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-ESPECIE   0.03       0.00       0.97     0.00     0.00        0.00        0.00\n",
      "B-CORTE     1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-CORTE     0.23       0.00       0.00     0.00     0.77        0.00        0.00\n",
      "B-DIMENSAO  1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-DIMENSAO  0.92       0.08       0.00     0.00     0.00        0.00        0.00\n",
      "weights     0.85       0.87       0.90     0.83     0.83        0.83        0.83\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "hmm.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_voter = skweak.voting.SequentialMajorityVoter(\n",
    "    \"maj_voter\",\n",
    "    labels=[\"ESPECIE\", \"CORTE\", \"DIMENSAO\"],\n",
    "    initial_weights=initial_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_docs_train = list(hmm.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_docs_train = list(maj_voter.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HMM Annotations ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7FFFD4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    angelim verm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " aproveitamento \n",
       "<mark class=\"entity\" style=\"background: #FFD700; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ripas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Majority Voter Annotations ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7FFFD4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    angelim verm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " aproveitamento \n",
       "<mark class=\"entity\" style=\"background: #FFD700; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ripas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render_skweak_entities(doc, annotator_name: str, colors: dict = None):\n",
    "    \"\"\"Render entities from skweak annotations with colored highlighting\"\"\"\n",
    "    \n",
    "    # Get entities from specified annotator\n",
    "    entities = doc.spans.get(annotator_name, [])\n",
    "    \n",
    "    # Default color scheme for your entities\n",
    "    default_colors = {\n",
    "        \"ESPECIE\": \"#7FFFD4\",  # Aquamarine\n",
    "        \"CORTE\": \"#FFD700\",    # Gold\n",
    "        \"DIMENSAO\": \"#FFB6C1\"  # Pink\n",
    "    }\n",
    "    \n",
    "    # Ensure colors is never None\n",
    "    colors = colors or default_colors\n",
    "    \n",
    "    # Prepare entities for visualization\n",
    "    displacy_data = {\n",
    "        \"text\": doc.text,\n",
    "        \"ents\": [\n",
    "            {\n",
    "                \"start\": ent.start_char,\n",
    "                \"end\": ent.end_char,\n",
    "                \"label\": ent.label_\n",
    "            } for ent in entities\n",
    "        ],\n",
    "        \"title\": None\n",
    "    }\n",
    "\n",
    "    # Render with specified colors (ensure empty dict if no colors)\n",
    "    displacy.render(\n",
    "        [displacy_data],\n",
    "        style=\"ent\",\n",
    "        manual=True,\n",
    "        jupyter=True,\n",
    "        options={\"colors\": colors or {}}  # Key fix here\n",
    "    )\n",
    "\n",
    "# Usage\n",
    "doc = spacy_docs_train[0]\n",
    "\n",
    "print(\"=== HMM Annotations ===\")\n",
    "render_skweak_entities(doc, \"hmm\")\n",
    "\n",
    "\n",
    "print(\"=== Majority Voter Annotations ===\")\n",
    "render_skweak_entities(doc, \"maj_voter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Se\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "docs_sem_entidades = [\n",
    "    doc for doc in spacy_docs_train[:50] \n",
    "    if len(doc.spans.get(\"hmm\", [])) == 0\n",
    "]\n",
    "\n",
    "df_sem_entidades_hmm = pd.DataFrame({\n",
    "    \"text\": [doc.text for doc in docs_sem_entidades],\n",
    "    \"num_entidades\": [len(doc.spans.get(\"hmm\", [])) for doc in docs_sem_entidades]\n",
    "})\n",
    "\n",
    "df_sem_entidades_maj_voter = pd.DataFrame({\n",
    "    \"text\": [doc.text for doc in docs_sem_entidades],\n",
    "    \"num_entidades\": [len(doc.spans.get(\"maj_voter\", [])) for doc in docs_sem_entidades]\n",
    "})\n",
    "\n",
    "print(len(df_sem_entidades_hmm))\n",
    "print(len(df_sem_entidades_maj_voter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_entidades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jogo cozinha em madeira</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conjunto de utensilios para cozi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>palitos p/ dentes aurea gde</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pv-kit colher / espatula em made</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text  num_entidades\n",
       "0           jogo cozinha em madeira              0\n",
       "1  conjunto de utensilios para cozi              0\n",
       "2       palitos p/ dentes aurea gde              0\n",
       "3  pv-kit colher / espatula em made              0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sem_entidades_hmm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_entidades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jogo cozinha em madeira</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conjunto de utensilios para cozi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>palitos p/ dentes aurea gde</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pv-kit colher / espatula em made</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text  num_entidades\n",
       "0           jogo cozinha em madeira              0\n",
       "1  conjunto de utensilios para cozi              0\n",
       "2       palitos p/ dentes aurea gde              0\n",
       "3  pv-kit colher / espatula em made              0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sem_entidades_maj_voter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Parece que ambos agregadores conseguiram explorar bem o conjunto de dados e identificar ao menos uma entidade quando o produto era realmente de madeira serrada.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to artefatos/spacy_docs_train_annotated.bin...done\n"
     ]
    }
   ],
   "source": [
    "from skweak.utils import docbin_writer\n",
    "docbin_writer(spacy_docs_train, \"artefatos/spacy_docs_train_annotated.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utilizar as Weak Labels para produzir um novo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Agora preciso utilizar as funções de rotulagem que implementei até agora para fazer o fine-tuning de um modelo Bert pré-treinado capaz de generalizar as regras 'hard' do Gazetteer e da heurística que criei, além das funções agregadoras.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades: [('angelim verm', 'ESPECIE'), ('ripas', 'CORTE')]\n"
     ]
    }
   ],
   "source": [
    "from skweak.utils import docbin_reader\n",
    "\n",
    "spacy_docs_train = list(docbin_reader(\"artefatos/spacy_docs_train_annotated.bin\", \n",
    "                                     spacy_model_name=\"pt_core_news_lg\"))\n",
    "\n",
    "doc = spacy_docs_train[0]\n",
    "print(\"Entidades:\", [(ent.text, ent.label_) for ent in doc.spans[\"hmm\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_iob(docs, annotator_name=\"hmm\"):\n",
    "    \"\"\"Convert skweak annotations to IOB format\"\"\"\n",
    "    iob_data = []\n",
    "    for doc in docs:\n",
    "        tokens = [token.text for token in doc]\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "        \n",
    "        # Process each entity span\n",
    "        for span in doc.spans[annotator_name]:\n",
    "            label = span.label_\n",
    "            start = span.start\n",
    "            end = span.end\n",
    "            \n",
    "            labels[start] = f\"B-{label}\"\n",
    "            for i in range(start+1, end):\n",
    "                labels[i] = f\"I-{label}\"\n",
    "                \n",
    "        iob_data.append({\"tokens\": tokens, \"labels\": labels, \"text\": doc.text})\n",
    "    \n",
    "    return iob_data\n",
    "\n",
    "train_data = convert_to_iob(spacy_docs_train, annotator_name=\"hmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_argilla_to_iob(argilla_data):\n",
    "    \"\"\"Convert Argilla format to IOB format\"\"\"\n",
    "    iob_data = []\n",
    "    \n",
    "    for example in argilla_data:\n",
    "        text = example[\"text\"]\n",
    "        entities = example.get(\"span_label.responses\", [])\n",
    "        \n",
    "        if isinstance(entities, str):\n",
    "            try:\n",
    "                clean_str = entities.replace(\"array(\", \"\").replace(\", dtype=object)\", \"\")\n",
    "                entities = ast.literal_eval(clean_str)\n",
    "            except:\n",
    "                entities = []\n",
    "\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "\n",
    "        for entity in entities:\n",
    "            if not isinstance(entity, dict):\n",
    "                continue\n",
    "\n",
    "            start = entity[\"start\"]\n",
    "            end = entity[\"end\"]\n",
    "            label = entity[\"label\"]\n",
    "\n",
    "            start_token = None\n",
    "            for token in doc:\n",
    "                if token.idx >= start:\n",
    "                    break\n",
    "                start_token = token.i\n",
    "            if start_token is None:\n",
    "                start_token = 0\n",
    "\n",
    "            end_token = None\n",
    "            for token in doc:\n",
    "                if token.idx + len(token) > end:\n",
    "                    break\n",
    "                end_token = token.i\n",
    "            if end_token is None:\n",
    "                end_token = len(doc) - 1\n",
    "\n",
    "            if start_token <= end_token:\n",
    "                labels[start_token] = f\"B-{label}\"\n",
    "                for i in range(start_token + 1, end_token + 1):\n",
    "                    labels[i] = f\"I-{label}\"\n",
    "\n",
    "        iob_data.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"labels\": labels,\n",
    "            \"text\": text\n",
    "        })\n",
    "    \n",
    "    return iob_data\n",
    "\n",
    "val_data = convert_argilla_to_iob(df_val.to_dict(\"records\"))\n",
    "test_data = convert_argilla_to_iob(df_test.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Casting the dataset: 100%|██████████| 21010/21010 [00:05<00:00, 3868.45 examples/s]\n",
      "Casting the dataset: 100%|██████████| 282/282 [00:00<00:00, 31277.60 examples/s]\n",
      "Casting the dataset: 100%|██████████| 282/282 [00:00<00:00, 21532.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Sequence, ClassLabel, Value\n",
    "\n",
    "labels = [\"O\", \"B-ESPECIE\", \"I-ESPECIE\", \"B-CORTE\", \"I-CORTE\", \"B-DIMENSAO\", \"I-DIMENSAO\"]\n",
    "\n",
    "dataset_train = Dataset.from_list(train_data).cast(Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    \"text\": Value(\"string\")\n",
    "}))\n",
    "dataset_val = Dataset.from_list(val_data).cast(Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    \"text\": Value(\"string\")\n",
    "}))\n",
    "dataset_test = Dataset.from_list(test_data).cast(Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    \"text\": Value(\"string\")\n",
    "}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 21010/21010 [00:01<00:00, 16734.08 examples/s]\n",
      "Map: 100%|██████████| 282/282 [00:00<00:00, 14777.72 examples/s]\n",
      "Map: 100%|██████████| 282/282 [00:00<00:00, 14778.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset_train = dataset_train.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_dataset_val = dataset_val.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_dataset_test = dataset_test.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, max_length=512, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='825' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [825/825 09:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>2.085716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>2.275171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091500</td>\n",
       "      <td>2.446688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>2.550125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>2.629310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=825, training_loss=0.13126839272452123, metrics={'train_runtime': 594.7532, 'train_samples_per_second': 176.628, 'train_steps_per_second': 1.387, 'total_flos': 2064217244832072.0, 'train_loss': 0.13126839272452123, 'epoch': 5.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"neuralmind/bert-base-portuguese-cased\",\n",
    "    num_labels=len(labels),\n",
    "    id2label={i: label for i, label in enumerate(labels)},\n",
    "    label2id={label: i for i, label in enumerate(labels)}\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./artefatos/bert-base-ner-true-labels\",  # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    per_device_train_batch_size=128,  # Batch size for training\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=1,  # Accumulate gradients over multiple steps\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    "    bf16=True,  # Use bfloat16 precision for training (if supported by hardware)\n",
    "    fp16=False,  # Use mixed precision training with FP16 (if supported by hardware)\n",
    "    save_total_limit=1,  # Limit the total number of saved checkpoints\n",
    "    logging_steps=1,  # Log training metrics every step\n",
    "    eval_steps=1,  # Evaluate the model every step\n",
    "    save_steps=1,  # Save the model every step\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to determine the best model\n",
    "    greater_is_better=False,  # Higher metric value is better\n",
    "    logging_strategy=\"steps\",  # Log metrics at each step\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    do_train=True,  # Perform training\n",
    "    do_eval=True,  # Perform evaluation\n",
    "    push_to_hub=False,  # Do not push the model to the Hugging Face Hub\n",
    "    learning_rate=1e-5,  # Learning rate for the optimizer\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Parece que o modelo não foi capaz de generalizar. Mesmo após fazer alguns testes modificando os parâmteros de treino, como o learning rate, per_device_train_batch_size, weight_decay, eu não fui capaz de encontrar uma combinação que diminuísse a Validation Loss.<br>\n",
    "Mesmo assim vamos analizar o comportamento desse modelo com alguns exemplos e no nosso conjunto de teste.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities detected:\n",
      "ang -> ESPECIE\n",
      "##eli -> ESPECIE\n",
      "##m ver -> ESPECIE\n",
      "ri -> CORTE\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"angelim verm aproveitamento ripas\" ## Exemplo do conjunto de treino, errou em ri ## pas\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "print(\"Entities detected:\")\n",
    "for entity in results:\n",
    "    print(f\"{entity['word']} -> {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_teste = tokenized_dataset_test['text'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = ner_pipeline(textos_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities detected:\n",
      "vig -> CORTE\n"
     ]
    }
   ],
   "source": [
    "print(\"Entities detected:\")\n",
    "for entity in results_list[0]:\n",
    "    print(f\"{entity['word']} -> {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities detected:\n",
      "cai -> CORTE\n",
      "##bro -> CORTE\n",
      "mis -> ESPECIE\n",
      "##to -> ESPECIE\n",
      "5x3. 5 -> DIMENSAO\n"
     ]
    }
   ],
   "source": [
    "print(\"Entities detected:\")\n",
    "for entity in results_list[1]:\n",
    "    print(f\"{entity['word']} -> {entity['entity_group']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
