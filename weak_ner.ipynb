{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Essa tarefa tem como objetivo demonstrar uma Pipeline de Supervisão Fraca.<br>\n",
    "Para essa tarefa eu escolhi utilizar o conjunto de dados da minha primeira tarefa no meu estágio, que comecei a um ano atrás.<br>\n",
    "Essa tarefa tem o objetivo encontrar as espécies, o tipo do corte e as dimensões das descrições de madeira.<br>\n",
    "Além de ser um trabalho de MLP, que na época eu não tinha nenhuma experiência, também teve a grande dificuldade dos dados não estarem rotulados.<br>\n",
    "A minha solução final envolveu a construção de um conjunto de treino e validação manual e utilizar uma LLM com um schema definido para retornar a espécie, o corte e as dimensões.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Temos algumas características que tornaram esse problema difícil para uma LLM, o primeiro, as descrições de produto não são estruturadas, além de não existir nenhum tipo de ordem para cada uma das classes de interesse também podemos ter casos em que essas classes estão faltando, como por exemplo uma descrição possuir a espécie, o corte, mas não possuir a dimensão.<br>\n",
    "Exemplo: \"tabua de massaranduba.\"<br>\n",
    "Um outro problema são as situações em que uma descrição possui múltiplos de uma mesma classe.<br>\n",
    "Exemplo: \"tabua/viga de massaranduba.\"<br>\n",
    "Um problema mais simples são erros ortográficos, sinônimos e nomes ciêntificos da espécie, existem múltiplas maneiras de chamar um mesmo produto que faz sentido no contexto de produtos de madeira, mas são palavras completamente diferentes em outros contextos.<br>\n",
    "Exemplo1: \"tabua de cetim.\"<br>\n",
    "Exemplo2: \"tabua de pau amarelo.\" -- Sinônimo<br>\n",
    "Exemplo3: \"tabua de cedrelinga cateniformis.\" -- Nome Ciêntifico<br>\n",
    "Além de problemas intrísecos ao Dataset, como as descrições de outros produtos que não são madeira com o NCM na nota fiscal como produto de madeira. Esses foram as maiores dificuldades que impactaram a qualidade da solução utilizanção LLM.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Essa atividade se mostrou uma boa oportunidade para revisitar esse problema.<br>\n",
    "Agora que tenho mais conhecimento na área de NLP, como eu posso solucionar essa tarefa?<br>\n",
    "Para isso eu vou partir do ponto em que eu reduzi o meu Dataset não rotulado para manter apenas as descrições únicas.<br>\n",
    "Essas descrições já passaram por um processo de pré-processamento simples, como: tornar tudo minúsculo e substituir acentos pelas letras normais.<br>\n",
    "Esse conjunto inicial é \"madeiraDescricao.csv\" e eu optei por manter apenas a coluna textual para evitar compartilhar dados sensíveis da nota.<br>\n",
    "Como os meus dados não estão rotulados essa é uma ótima oportunidade para utilizar o que aprendi com Weak Learning.<br>\n",
    "Primeiro preciso criar um conjunto de dados rotulados que será dividido em validação e teste. Mas para criar esse conjunto eu primeiro preciso entender como resolverei esses problemas.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "A solução que decidi seguir foi a implementação de uma tarefa de NER, em que as entidades serão as <mark>ESPECIE</mark>, <mark>CORTE</mark> E <mark>DIMENSAO</mark>.<br>\n",
    "Eu escolhi selecionar essa solução devido aos problemas mencionados, as multiplicidade das entidades, ou falta de alguma delas não seria um problema, além disso acredito que os problemas de sinônimos e nomes cientificos também são de fácil solução para essa tarefa.<br>\n",
    "Então para começar eu selecionei uma amostra aleatória de 600 instâncias e as removi do conjunto não rotulado, para fazer o rótulo dessas entidades eu optei por utilizar o Argilla devido a possibilidade de subir um container localmente, evitando o vazamento desses dados, uma etapa que seria necessária no meu ambiente do estágio.<br>\n",
    "O conjunto de dados que será rotulado é \"labedMadeiraDescricao.csv\" e o restante está no conjunto \"unlabeledMadeiraDescricao.csv\".<br>\n",
    "Esse foi o resultado final 564 dos 600 erão produtos de madeira e foram rotulados, o restante não será utilizado.<br>\n",
    "<img src=\"images/argilla.jpg\" alt=\"Argilla\" width=\"400\"><br>\n",
    "Agora que temos o conjunto de avaliação rotulado, \"labeledTokenClassification.csv\" podemos começar a resolver a tarefa.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "df_val_test = pd.read_csv('dataset/labeledTokenClassification.csv')\n",
    "df_unlabeled = pd.read_csv('dataset/unlabeledMadeiraDescricao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test primeiros 100\n",
    "df_unlabeled = df_unlabeled[:100]\n",
    "len(df_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Selecionar apenas as instâncias com o status de completo, para evitar utilizar as descrições que não são produtos de madeira.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_test = df_val_test[df_val_test.status == 'completed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Vamos separar em dois conjuntos, um para validação e outro para teste.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_val, df_test = train_test_split(\n",
    "    df_val_test,\n",
    "    test_size=0.5,       \n",
    "    random_state=1, # Necessário a mudança caso utilizar o conjunto de teste\n",
    "    shuffle=True         \n",
    ")\n",
    "print(len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversão para Objetos do Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skweak\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to artefatos/spacy_docs_train.bin...done\n",
      "Write to artefatos/spacy_docs_dev.bin...done\n",
      "Write to artefatos/spacy_docs_test.bin...done\n"
     ]
    }
   ],
   "source": [
    "spacy_docs_train = list(nlp.pipe(df_unlabeled[\"text\"].values))\n",
    "\n",
    "spacy_docs_dev = list(nlp.pipe(df_val[\"text\"].values))\n",
    "\n",
    "spacy_docs_test = list(nlp.pipe(df_test[\"text\"].values))\n",
    "\n",
    "skweak.utils.docbin_writer(spacy_docs_train, \"artefatos/spacy_docs_train.bin\")\n",
    "skweak.utils.docbin_writer(spacy_docs_dev, \"artefatos/spacy_docs_dev.bin\")\n",
    "skweak.utils.docbin_writer(spacy_docs_test, \"artefatos/spacy_docs_test.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training documents\n",
    "spacy_docs_train = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_train.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "\n",
    "# Load the validation documents\n",
    "spacy_docs_dev = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_dev.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "\n",
    "# Load the test documents\n",
    "spacy_docs_test = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_dev.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "\n",
    "# Convert the loaded documents to lists\n",
    "# This step ensures that the documents are in a list format, which is easier to work with in subsequent steps\n",
    "spacy_docs_train = list(spacy_docs_train)\n",
    "spacy_docs_dev = list(spacy_docs_dev)\n",
    "spacy_docs_test = list(spacy_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[angelim verm aproveitamento ripas, caibro mixto 4,5mt]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_docs_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Criação de funções de rotulagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Vamos precisar fazer funções de rotulagem para cada uma das entidades de interesse, <mark>ESPECIE</mark>, <mark>CORTE</mark>, <mark>DIMENSAO</mark>.<br>\n",
    "Para as funções de rotulagem de ESPECIE, podemos começar com o seu nome popular e sinônimos. Eu já tenho um dicionário que desenvolvi na época com as espécies mais relavantes para o nosso domínio no mercado do estado do RN.<br>\n",
    "Por isso irei partir dele e adicionar alguns casos simples para compensar a falta da função de Levenshtein que utilizei na época para aceitar pequenas alterações na variação das palavras.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "madeiras_dicionario = {\n",
    "    'acacia': ['angico'],\n",
    "    'acapu': ['angelim de folha larga'],\n",
    "    'amapa': [],\n",
    "    'amescla': ['amesclao', 'breu', 'morcegueira', 'sucuruba'],\n",
    "    'andiroba': ['angirova', 'carapa'],\n",
    "    'angelim amargoso': ['angelim amarelo', 'fava', 'fava amargosa', 'faveira'],\n",
    "    'angelim pedra': ['angelim verm', 'angelim vermelho', 'angelim falso', 'faveira ferro'],\n",
    "    'angelim indefinido': ['angelim', 'angelin', 'ang', 'ange'],\n",
    "    'cedrinho': ['quaruba', 'quaruvatinga', 'mandioqueira', 'quaruba vermelha'],\n",
    "    'cedro': [],\n",
    "    'cerejeira': [],\n",
    "    'cetim': ['pau amarelo', 'pau cetim', 'piquia cetim', 'garapa', ' muirajuba'],\n",
    "    'coco pau': ['baru'],\n",
    "    'copaiba': ['cupauba', 'copauva'],\n",
    "    'cumaru': ['cumaru de folha grande', 'cumbar', 'cumbaru roxo', 'cumbaru'],\n",
    "    'cupiuba': ['copiuba', 'copiuva'],\n",
    "    'eucalipto': ['euc', 'ecalipto', 'eucalipito'],\n",
    "    'freijo': [],\n",
    "    'garapeira': [],\n",
    "    'goiabao': ['abiu', 'abiurana'],\n",
    "    'guajara': ['currupixa', 'angico-bravo', 'angico-cedro', 'angico-rosa', 'curupia'],\n",
    "    'ipe': [],\n",
    "    'itauba': [],\n",
    "    'jatoba': ['jatai', 'utai', 'jati'],\n",
    "    'jequitiba': ['cachimbeiro', 'estopeira', 'coatinga', 'bingueiro'],\n",
    "    'louro amarelo': ['louro pardo, louro abacate', 'louro inhamui'],\n",
    "    'louro canela': ['louro rosa', 'louro gamela', 'louro vermelho'],\n",
    "    'louro indefinido': ['louro'],\n",
    "    'louro preto': [],\n",
    "    'marupa': [],\n",
    "    'massaranduba': ['macaranduba', 'masaranduba', 'maparajuba', 'aparaui', 'paraju', 'mass', 'massa'],\n",
    "    'matamata': [],\n",
    "    'melanceira': ['sucupira-pepino'],\n",
    "    'mogno': [],\n",
    "    'muiracatiara': [],\n",
    "    'para para': ['morototo', 'mandioqueira'],\n",
    "    'pau louro': [],\n",
    "    'pau mulato': [],\n",
    "    'peroba': ['amapa amargoso', 'parahancornia'],\n",
    "    'pinho': ['araucaria'],\n",
    "    'pinus': ['pinheiro', 'pinnus'],\n",
    "    'piquia': ['piqui', 'piquiarana', 'pequia'],\n",
    "    'roxinho': ['pau roxo', 'guarabu', 'pau ferro'],\n",
    "    'sapucaia': ['jarana'],\n",
    "    'sucupira': ['cutiba', 'sapupira', 'sebepira'],\n",
    "    'sumauma': ['ceiba'],\n",
    "    'tanimbuca': ['amarelao', 'mirindiba', 'embiridiba', 'cuiarana ,carana', 'mangue'],\n",
    "    'tatajuba': ['amarelo', 'amarelinho', 'garrote', 'bagaceira'],\n",
    "    'tauari': ['imbirema', 'estopeiro', 'toari'],\n",
    "    'taxi': [],\n",
    "    'timborana': ['timbauba', 'angico'],\n",
    "    'uxi': ['axua', 'cumate', 'paruru'],\n",
    "    'mista': ['misto', 'mixta', 'mix'],\n",
    "    'virola': ['ucuuba']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Além dos nomes populares e sinônimos também podemos fazer funções de rotulagem para a entidade ESPECIE utilizando o nome ciêntifico da espécie de madeira.<br>\n",
    "Essas funções não serão tão valiosas pela característica das descrições. Normalmente quando o nome ciêntifico está na descrição do produto ele está acompanhado do nome popular.<br>\n",
    "Mesmo assim eu achei relevante coletar essas entidades, porém irei utilizar um outro dicionário para tal, dessa forma poderemos diferenciar qual a fonte de rotulagem para a ESPECIE.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cientifico_madeiras_dicionario = {\n",
    "    'acapu': ['vouacapoua americana', 'vouacapoua'],\n",
    "    'amapa': ['parahancornia amapa', 'parahancornia'],\n",
    "    'amescla': ['protium heptaphyllum', 'protium', 'heptaphyllum'],\n",
    "    'andiroba': ['carapa guianensis', 'carapa', 'guianensis'],\n",
    "    'angelim amargoso': ['vatairea sericea', 'vatairea', 'sericea'],\n",
    "    'angelim pedra': ['hymenolobium petraeum', 'hymenolobium', 'petraeum'],\n",
    "    'angelim indefinido': ['dinizia excelsa', 'dinizia', 'excelsa'],\n",
    "    'cedrinho': ['erisma uncinatum', 'erisma', 'uncinatum'],\n",
    "    'cedro': ['cedrela fissilis', 'cedrela', 'fissilis'],\n",
    "    'cerejeira': ['amburana cearensis', 'amburana', 'cearensis'],\n",
    "    'cetim': ['cedrelinga cateniformis', 'cedrelinga', 'cateniformis'],\n",
    "    'coco pau': ['orbygnia phalerata', 'orbygnia', 'phalerata'],\n",
    "    'copaiba': ['copaifera langsdorffii', 'copaifera', 'langsdorffii'],\n",
    "    'cumaru': ['dipteryx odorata', 'dipteryx', 'odorata'],\n",
    "    'cupiuba': ['goupia glabra', 'goupia', 'glabra'],\n",
    "    'eucalipto': ['eucalyptus'],\n",
    "    'freijo': ['cordia goeldiana', 'cordia', 'goeldiana'],\n",
    "    'garapeira': ['apuleia leiocarpa', 'apuleia', 'leiocarpa'],\n",
    "    'goiabao': ['qualea'],\n",
    "    'guajara': ['terminalia amazonia', 'terminalia', 'amazonia'],\n",
    "    'ipe': ['handroanthus'],\n",
    "    'itauba': ['mezilaurus itauba', 'mezilaurus', 'itauba'],\n",
    "    'jatoba': ['hymenaea courbaril', 'hymenaea', 'courbaril'],\n",
    "    'jequitiba': ['cariniana legalis', 'cariniana', 'legalis'],\n",
    "    'louro amarelo': ['ocotea'],\n",
    "    'louro canela': ['nectandra'],\n",
    "    'louro indefinido': ['ocotea'],\n",
    "    'louro preto': ['ocotea porosa', 'ocotea', 'porosa'],\n",
    "    'marupa': ['simarouba amara', 'simarouba', 'amara'],\n",
    "    'massaranduba': ['manilkara huberi', 'manilkara', 'huberi'],\n",
    "    'matamata': ['eschweilera'],\n",
    "    'melanceira': ['trattinnickia burserifolia', 'trattinnickia', 'burserifolia'],\n",
    "    'mogno': ['swietenia macrophylla', 'swietenia', 'macrophylla'],\n",
    "    'muiracatiara': ['astronium lecointei', 'astronium', 'lecointei'],\n",
    "    'para para': ['jacaranda copaia', 'jacaranda', 'copaia'],\n",
    "    'pau louro': ['ocotea'],\n",
    "    'pau mulato': ['calycophyllum spruceanum', 'calycophyllum', 'spruceanum'],\n",
    "    'peroba': ['aspidosperma'],\n",
    "    'pinho': ['araucaria angustifolia', 'araucaria', 'angustifolia'],\n",
    "    'pinus': ['pinus'],\n",
    "    'piquia': ['caryocar villosum', 'caryocar', 'villosum'],\n",
    "    'roxinho': ['peltogyne'],\n",
    "    'sapucaia': ['lecythis pisonis', 'lecythis', 'pisonis'],\n",
    "    'sucupira': ['bowdichia virgilioides', 'bowdichia', 'virgilioides'],\n",
    "    'sumauma': ['ceiba pentandra', 'ceiba', 'pentandra'],\n",
    "    'tanimbuca': ['buchenavia tetraphylla', 'buchenavia', 'tetraphylla'],\n",
    "    'tatajuba': ['bagassa guianensis', 'bagassa', 'guianensis'],\n",
    "    'tauari': ['couratari'],\n",
    "    'taxi': ['sclerolobium'],\n",
    "    'timborana': ['enterolobium schomburgkii', 'enterolobium', 'schomburgkii'],\n",
    "    'uxi': ['endopleura uchi', 'endopleura', 'uchi'],\n",
    "    'virola': ['virola']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Para a entidade de <mark>CORTE</mark> podemos fazer um processo bem similar ao do primeiro dicionário de madeira, utilizando o nome popular do tipo de corte e o seu sinônimo.<br>\n",
    "Similar ao caso de dicionário também teremos que compensar pelo fator de que eu não estou usando Levenshtein, por isso precisamos aceitar erros comuns\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "apresentacoes_dicionario ={\n",
    "                            'alizar': ['alisar'],\n",
    "                            'barrote':[],\n",
    "                            'caibro':['caibrinho'],\n",
    "                            'frechal': [],\n",
    "                            'janela': ['janelao'],\n",
    "                            'linha':[],\n",
    "                            'mourao': [],\n",
    "                            'quadrado': ['quadradro', 'bloco', 'file', 'quadradinho'],\n",
    "                            'piquete': [],\n",
    "                            'pontalete': [],\n",
    "                            'porta': ['portal', 'aduela'],\n",
    "                            'prancha':['pranchoes', 'pranchinha', 'pranchao'],\n",
    "                            'ripa':['ripao'],\n",
    "                            'tabua':['tauba'],\n",
    "                            'taipa':[],\n",
    "                            'sarrafo':['serrafo'],\n",
    "                            'viga':['virga','vigota', 'poste']\n",
    "                            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "E também irei complementar com o plural dos tipos de corte que terminam em vogal.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adc_plural(apresentacoes_dict):\n",
    "    updated_dict = {}\n",
    "    for key, synonyms in apresentacoes_dict.items():\n",
    "        pluralized_synonyms = []\n",
    "        \n",
    "        if key[-1].lower() in{'a', 'e', 'i', 'o', 'u'}:\n",
    "            plural = key + \"s\"\n",
    "        \n",
    "            if plural not in synonyms:\n",
    "                pluralized_synonyms.append(plural)\n",
    "\n",
    "        for term in synonyms:\n",
    "            pluralized_synonyms.append(term)\n",
    "            \n",
    "            if term and term[-1].lower() in {'a', 'e', 'i', 'o', 'u'}:\n",
    "                if \" \" in term:\n",
    "                    parts = term.split()\n",
    "                    parts[-1] += \"s\"\n",
    "                    plural = \" \".join(parts)\n",
    "                else:\n",
    "                    plural = term + \"s\"\n",
    "                pluralized_synonyms.append(plural)\n",
    "        \n",
    "        seen = set()\n",
    "        unique_synonyms = [t for t in pluralized_synonyms \n",
    "                          if not (t in seen or seen.add(t))]\n",
    "        \n",
    "        updated_dict[key] = unique_synonyms\n",
    "        \n",
    "    return updated_dict\n",
    "\n",
    "apresentacoes_dicionario = adc_plural(apresentacoes_dicionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "A coleta das dimensões do produto de madeira foi um dos maiores desafios, porque as descrições normalmente não seguem um padrão, o contribuinte quando escreve essa informação muitas vezes não coloca a dimensão associada a um número, como por exemplo \"linha massaranduba 5 x 12\", conseguimos assumir logicamente ao olhar para esse exemplo que não será 5 por 12 metros e muito provavelmente essa medida está em centímetros, existem alguns exemplos em que essa dimensão é ambígua, mas no geral conseguimos inferir.<br>\n",
    "Além disso, outro problema é que esses números podem estar soltos no texto, como em descrições do tipo \"linha 5x28 cm angelim c/ 6,0 mt\", então precisaremos considerar diversos tipos de padrões ao fazer as funções anotadoras para a entidade de <mark>DIMENSAO</mark>.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "## (PALAVRA) pode conter uma palavra | PALAVRA precisa conter uma palavra.\n",
    "padroes = [\n",
    "    # NUM X NUM X NUM (PALAVRA) -- 2.5 x 3 x 4 cm\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]*)', \n",
    "    # NUM PALAVRA X NUM PALAVRA X NUM PALAVRA -- 2.5 cm x 3 m x 4 mm\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\b', \n",
    "    # NUM X NUM (PALAVRA) -- 2 x 3 cm\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]*)', \n",
    "    # NUM PALAVRA X NUM PALAVRA -- 2cm x 3m\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\b',\n",
    "    # NUM X NUM -- 2.5 x 3\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\b',\n",
    "    # NUM (m, cm, mm, metro, metros, pol) -- 2 metros | Mais restrito para evitar coletar número de série ou lixo nas descrições\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*(m|cm|mm|metro|metros|pol)\\b'\n",
    "]\n",
    "\n",
    "padroes_compilados = [re.compile(padrao, re.IGNORECASE) for padrao in padroes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Vamos criar uma heurística para a dimensão, se um dos padrões acontece inferimos que aquele termo está representando uma dimensão.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_dimensao(doc):\n",
    "    text = doc.text\n",
    "    for padrao in padroes_compilados:\n",
    "        for match in padrao.finditer(text):\n",
    "            start_char, end_char = match.span()\n",
    "            span = doc.char_span(start_char, end_char, alignment_mode=\"expand\")\n",
    "            if span:\n",
    "                yield span.start, span.end, \"DIMENSAO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skweak import heuristics\n",
    "dimensao_annotator = heuristics.FunctionAnnotator(\n",
    "    \"dimensao_annotator\", \n",
    "    detectar_dimensao,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skweak.gazetteers import GazetteerAnnotator, Trie\n",
    "from skweak.base import CombinedAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "especie_terms = []\n",
    "for key, synonyms in madeiras_dicionario.items():\n",
    "    especie_terms.append(key)\n",
    "    especie_terms.extend(synonyms)\n",
    "\n",
    "cientifico_especie_terms = []\n",
    "for key, cientifico in cientifico_madeiras_dicionario.items():\n",
    "    cientifico_especie_terms.extend(cientifico)\n",
    "\n",
    "corte_terms = []\n",
    "for key, synonyms in apresentacoes_dicionario.items():\n",
    "    corte_terms.append(key)\n",
    "    corte_terms.extend(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trie(term_list):\n",
    "    \"\"\"Convert a list of terms into a trie of token sequences\"\"\"\n",
    "    trie = Trie()\n",
    "    for term in term_list:\n",
    "        tokens = term.split()\n",
    "        trie.add(tokens)\n",
    "    return trie\n",
    "\n",
    "especie_trie = build_trie(especie_terms)\n",
    "cientifico_especie_trie = build_trie(cientifico_especie_terms)\n",
    "corte_trie = build_trie(corte_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['vouacapoua'], ['vouacapoua', 'americana'], ['parahancornia'], ['parahancornia', 'amapa'], ['protium'], ['protium', 'heptaphyllum'], ['heptaphyllum'], ['carapa'], ['carapa', 'guianensis'], ['guianensis'], ['vatairea'], ['vatairea', 'sericea'], ['sericea'], ['hymenolobium'], ['hymenolobium', 'petraeum'], ['petraeum'], ['dinizia'], ['dinizia', 'excelsa'], ['excelsa'], ['erisma'], ['erisma', 'uncinatum'], ['uncinatum'], ['cedrela'], ['cedrela', 'fissilis'], ['fissilis'], ['amburana'], ['amburana', 'cearensis'], ['cearensis'], ['cedrelinga'], ['cedrelinga', 'cateniformis'], ['cateniformis'], ['orbygnia'], ['orbygnia', 'phalerata'], ['phalerata'], ['copaifera'], ['copaifera', 'langsdorffii'], ['langsdorffii'], ['dipteryx'], ['dipteryx', 'odorata'], ['odorata'], ['goupia'], ['goupia', 'glabra'], ['glabra'], ['eucalyptus'], ['cordia'], ['cordia', 'goeldiana'], ['goeldiana'], ['apuleia'], ['apuleia', 'leiocarpa'], ['leiocarpa'], ['qualea'], ['terminalia'], ['terminalia', 'amazonia'], ['amazonia'], ['handroanthus'], ['mezilaurus'], ['mezilaurus', 'itauba'], ['itauba'], ['hymenaea'], ['hymenaea', 'courbaril'], ['courbaril'], ['cariniana'], ['cariniana', 'legalis'], ['legalis'], ['ocotea'], ['ocotea', 'porosa'], ['nectandra'], ['porosa'], ['simarouba'], ['simarouba', 'amara'], ['amara'], ['manilkara'], ['manilkara', 'huberi'], ['huberi'], ['eschweilera'], ['trattinnickia'], ['trattinnickia', 'burserifolia'], ['burserifolia'], ['swietenia'], ['swietenia', 'macrophylla'], ['macrophylla'], ['astronium'], ['astronium', 'lecointei'], ['lecointei'], ['jacaranda'], ['jacaranda', 'copaia'], ['copaia'], ['calycophyllum'], ['calycophyllum', 'spruceanum'], ['spruceanum'], ['aspidosperma'], ['araucaria'], ['araucaria', 'angustifolia'], ['angustifolia'], ['pinus'], ['caryocar'], ['caryocar', 'villosum'], ['villosum'], ['peltogyne'], ['lecythis'], ['lecythis', 'pisonis'], ['pisonis'], ['bowdichia'], ['bowdichia', 'virgilioides'], ['virgilioides'], ['ceiba'], ['ceiba', 'pentandra'], ['pentandra'], ['buchenavia'], ['buchenavia', 'tetraphylla'], ['tetraphylla'], ['bagassa'], ['bagassa', 'guianensis'], ['couratari'], ['sclerolobium'], ['enterolobium'], ['enterolobium', 'schomburgkii'], ['schomburgkii'], ['endopleura'], ['endopleura', 'uchi'], ['uchi'], ['virola']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cientifico_especie_trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "especie_annotator = GazetteerAnnotator(\"especie_annotator\", {\"ESPECIE\": especie_trie})\n",
    "cientifico_especie_annotator = GazetteerAnnotator(\"cientifico_especie_annotator\", {\"ESPECIE\": cientifico_especie_trie})\n",
    "corte_annotator = GazetteerAnnotator(\"corte_annotator\", {\"CORTE\": corte_trie})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<skweak.base.CombinedAnnotator at 0x24cac046110>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = CombinedAnnotator()\n",
    "combined.add_annotator(especie_annotator)\n",
    "combined.add_annotator(cientifico_especie_annotator)\n",
    "combined.add_annotator(corte_annotator)\n",
    "combined.add_annotator(dimensao_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">madeira serrada \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    massaranduba\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    manilkara huberi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       ") \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2.5 metro\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DIMENSAO</span>\n",
       "</mark>\n",
       " 2 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    vigas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       " de \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    10x15cm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DIMENSAO</span>\n",
       "</mark>\n",
       " e 3 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tabuas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       " de \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2,5x30x200cm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DIMENSAO</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"madeira serrada massaranduba (manilkara huberi) 2.5 metro 2 vigas de 10x15cm e 3 tabuas de 2,5x30x200cm\")\n",
    "combined(doc)\n",
    "skweak.utils.display_entities(doc, [\"especie_annotator\", \"cientifico_especie_annotator\", \"corte_annotator\", \"dimensao_annotator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESPECIE: ['massaranduba']\n",
      "ESPECIE: ['manilkara huberi']\n",
      "CORTE: ['vigas', 'tabuas']\n",
      "DIMENSAO: ['2,5x30x200cm', '10x15cm', '2,5x30x200', '2.5 metro']\n"
     ]
    }
   ],
   "source": [
    "print(\"ESPECIE:\", [ent.text for ent in doc.spans[\"especie_annotator\"]])\n",
    "print(\"ESPECIE:\", [ent.text for ent in doc.spans[\"cientifico_especie_annotator\"]])\n",
    "print(\"CORTE:\", [ent.text for ent in doc.spans[\"corte_annotator\"]])\n",
    "print(\"DIMENSAO:\", [ent.text for ent in doc.spans[\"dimensao_annotator\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Utilizando um exemplo do conjunto de treinamento.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    angelim verm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " aproveitamento \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ripas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = spacy_docs_train[0]\n",
    "\n",
    "combined(doc)\n",
    "\n",
    "skweak.utils.display_entities(doc, [\"especie_annotator\", \"cientifico_especie_annotator\", \"corte_annotator\", \"dimensao_annotator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Sequence\n",
    "\n",
    "class ESPECIE(BaseModel):\n",
    "    nome: Optional[str] = Field(None, description=\"Nome comercial da madeira.\")\n",
    "    nome_cientifico: Optional[str] = Field(None, description=\"Nome científico da madeira\")\n",
    "\n",
    "class CORTE(BaseModel):\n",
    "    nome: str = Field(..., description=\"Tipo do corte da madeira.\")\n",
    "\n",
    "class DIMENSAO(BaseModel):\n",
    "    medida: str = Field(..., description=\"Medidas do produto de madeira.\")\n",
    "\n",
    "class MadeiraInfo(BaseModel):\n",
    "    especies: Optional[List[ESPECIE]] = Field(default_factory=list, alias=\"ESPECIE\")\n",
    "    cortes: Optional[List[CORTE]] = Field(default_factory=list, alias=\"CORTE\")\n",
    "    dimensoes: Optional[List[DIMENSAO]] = Field(default_factory=list, alias=\"DIMENSAO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Você é um especialista em produtos de madeira serrada. Extraia entidades no seguinte formato JSON:\n",
    "{\n",
    "  \"ESPECIE\": [\n",
    "    {\"nome\": \"nome_comum\", \"nome_cientifico\": \"nome_cientifico\"}\n",
    "  ],\n",
    "  \"CORTE\": [\n",
    "    {\"nome\": \"tipo_corte\"}\n",
    "  ],\n",
    "  \"DIMENSAO\": [\n",
    "    {\"medida\": \"valor_dimensao\"}\n",
    "  ]\n",
    "}\n",
    "Regras:\n",
    "- Mantenha os valores exatos do texto\n",
    "- Use sempre arrays mesmo para único elemento\n",
    "Dicionário para os campos:\n",
    "- ESPECIE: \n",
    "  - nome: O nome comercial de um tipo de madeira. (ex.: massaranduba, pinus, angelim, jatoba, mogno)\n",
    "  - nome_cientifico: O nome cientrifico de um tipo de madeira. (ex.: manilkara huberi, dinizia excelsa, swietenia macrophylla)\n",
    "- CORTE: O formato ou tipo da peça de madeira. (ex.: viga, tabua, ripa, barrote, caibro)\n",
    "- DIMENSOES: As dimensões da peça de madeira, representam o comprimento, largura e altura da peça. (ex.: 2.5x3x4cm, 2cm x 1.5cm x 1m, 2x3cm, 1cm x 2.5m, 2.5x3, 2 metros)\n",
    "Exemplo entrada: \n",
    "\"madeira massaranduba (manilkara huberi) em vigas 10x15cm\"\n",
    "Exemplo saída:\n",
    "{\n",
    "  \"ESPECIE\": [{\"nome\": \"massaranduba\", \"nome_cientifico\": \"manilkara huberi\"}],\n",
    "  \"CORTE\": [{\"nome\": \"vigas\"}],\n",
    "  \"DIMENSAO\": [{\"medida\": \"10x15cm\"}]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "# def extract_wood_entities(text: str) -> MadeiraInfo:\n",
    "#     \"\"\"Extract entities using Ollama with structured JSON output\"\"\"\n",
    "#     response = ollama.chat(\n",
    "#         model='qwen3:4b',\n",
    "#         format='json',\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 'role': 'system',\n",
    "#                 'content': system_prompt\n",
    "#             },\n",
    "#             {\n",
    "#                 'role': 'user',\n",
    "#                 'content': text\n",
    "#             }\n",
    "#         ],\n",
    "#         options={'temperature': 0.3}\n",
    "#     )\n",
    "#     return MadeiraInfo.model_validate_json(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_entity_spans(text: str, entities: MadeiraInfo) -> list[dict]:\n",
    "#     \"\"\"Find exact text spans for each entity\"\"\"\n",
    "#     results = []\n",
    "    \n",
    "#     def add_matches(value: str, label: str):\n",
    "#         if not value:\n",
    "#             return\n",
    "#         for match in re.finditer(re.escape(value.lower()), text.lower()):\n",
    "#             start = text.lower().find(value.lower(), match.start(), match.end())\n",
    "#             end = start + len(value)\n",
    "#             results.append({\n",
    "#                 'start': start,\n",
    "#                 'end': end,\n",
    "#                 'label': label,\n",
    "#                 'text': text[start:end]\n",
    "#             })\n",
    "    \n",
    "#     for especie in entities.especies:\n",
    "#         add_matches(especie.nome, \"ESPECIE\")\n",
    "#         add_matches(especie.nome_cientifico, \"ESPECIE\")\n",
    "    \n",
    "#     for corte in entities.cortes:\n",
    "#         add_matches(corte.nome, \"CORTE\")\n",
    "    \n",
    "#     for dimensao in entities.dimensoes:\n",
    "#         add_matches(dimensao.medida, \"DIMENSAO\")\n",
    "    \n",
    "#     seen = set()\n",
    "#     unique_results = []\n",
    "#     for res in results:\n",
    "#         key = (res['start'], res['end'], res['label'])\n",
    "#         if key not in seen:\n",
    "#             seen.add(key)\n",
    "#             unique_results.append(res)\n",
    "    \n",
    "#     return sorted(unique_results, key=lambda x: x['start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text = \"ripa de cedro 1cm x 5 cm 2 m\"\n",
    "# entities = extract_wood_entities(text)\n",
    "# spans = find_entity_spans(text, entities)\n",
    "\n",
    "# print(\"Extracted entities:\")\n",
    "# for span in spans:\n",
    "#     print(f\"{span['text']} ({span['label']}) [{span['start']}-{span['end']}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "\n",
    "# # Define a dictionary to map entity labels to specific colors for visualization\n",
    "# colors = {\n",
    "#     \"ESPECIE\": \"#2ECC71\",  # Green color for 'Medicamento' entities\n",
    "#     \"CORTE\": \"#3498DB\",  # Blue color for 'Procedimento' entities\n",
    "#     \"DIMENSAO\": \"#E74C3C\",  # Red color for 'Doença' entities\n",
    "# }\n",
    "\n",
    "# # Define options for displacy visualization, including the entity labels and their corresponding colors\n",
    "# options = {\"ents\": list(colors.keys()), \"colors\": colors}\n",
    "\n",
    "# # Prepare the data for rendering by displacy\n",
    "# # 'render_data' is a list of dictionaries, each containing the text and its corresponding entities\n",
    "# render_data = [{\"text\": text, \"ents\": spans}]\n",
    "\n",
    "# # Render the entities in the text using displacy with the specified options\n",
    "# # 'style=\"ent\"' specifies that we are visualizing named entities\n",
    "# # 'manual=True' indicates that we are providing the entities manually\n",
    "# # 'jupyter=True' ensures the visualization is displayed correctly in a Jupyter notebook\n",
    "# displacy.render(render_data, style=\"ent\", manual=True, jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = df_unlabeled.text.iloc[0]\n",
    "# entities = extract_wood_entities(text)\n",
    "# spans = find_entity_spans(text, entities)\n",
    "\n",
    "# print(\"Extracted entities:\")\n",
    "# for span in spans:\n",
    "#     print(f\"{span['text']} ({span['label']}) [{span['start']}-{span['end']}]\")\n",
    "\n",
    "# render_data = [{\"text\": text, \"ents\": spans}]\n",
    "# displacy.render(render_data, style=\"ent\", manual=True, jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.tokens import Doc\n",
    "\n",
    "# def detectar_entidades_llm(doc: Doc):\n",
    "#     text = doc.text\n",
    "    \n",
    "#     try:\n",
    "#         entities = extract_wood_entities(text)\n",
    "#         spans = find_entity_spans(text, entities)\n",
    "        \n",
    "#         for span in spans:\n",
    "#             char_span = doc.char_span(\n",
    "#                 span['start'], \n",
    "#                 span['end'],\n",
    "#                 label=span['label'],\n",
    "#                 alignment_mode=\"expand\"\n",
    "#             )\n",
    "            \n",
    "#             if char_span and isinstance(char_span.start, int) and isinstance(char_span.end, int):\n",
    "#                 yield (char_span.start, char_span.end, span['label'])\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing document: {e}\")\n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_annotator = heuristics.FunctionAnnotator(\n",
    "#     \"llm_annotator\",\n",
    "#     detectar_entidades_llm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined.add_annotator(llm_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = df_unlabeled.text.iloc[0]\n",
    "# doc = nlp(text)\n",
    "# combined(doc)\n",
    "# skweak.utils.display_entities(doc, [\"especie_annotator\", \"cientifico_especie_annotator\", \"corte_annotator\", \"dimensao_annotator\", \"llm_annotator\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 characters of API key: sk-pr\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv(\n",
    "    override=True\n",
    ")\n",
    "\n",
    "api_key_preview = os.getenv(\"OPENAI_API_KEY\")[:5]\n",
    "print(f\"First 5 characters of API key: {api_key_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Você é um especialista em extração de entidades nomeadas com precisão excepcional. \"\n",
    "            \"Sua tarefa é identificar e extrair informações específicas do texto fornecido, seguindo estas diretrizes:\"\n",
    "            \"\\n\\n1. Extraia as informações exatamente como aparecem no texto, sem interpretações ou alterações.\"\n",
    "            \"\\n2. Se uma informação solicitada não estiver presente ou for ambígua, retorne null para esse campo.\"\n",
    "            \"\\n3. Mantenha-se estritamente dentro do escopo das entidades e atributos definidos no esquema fornecido.\"\n",
    "            \"\\n4. Preste atenção especial para manter a mesma ortografia, pontuação e formatação das informações extraídas.\"\n",
    "            \"\\n5. Não infira ou adicione informações que não estejam explicitamente presentes no texto.\"\n",
    "            \"\\n6. Se houver múltiplas menções da mesma entidade, extraia todas as ocorrências relevantes.\"\n",
    "            \"\\n7. Ignore informações irrelevantes ou fora do contexto das entidades solicitadas.\"\n",
    "            \"\\n\\nLembre-se: sua precisão e aderência ao texto original são cruciais para o sucesso desta tarefa.\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_openai = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# model_openai.invoke(\"Oi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_openai = prompt | model_openai.with_structured_output(\n",
    "    MadeiraInfo, include_raw=False, method = \"function_calling\",\n",
    ").with_retry(\n",
    "    stop_after_attempt=10,  # Retry up to 10 times in case of failure\n",
    "    wait_exponential_jitter=True,  # Add randomness to the wait time between retries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = model_openai.invoke({\"text\": df_unlabeled.text.iloc[0]})\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.44s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "batch_size = 100\n",
    "texts = df_unlabeled[\"text\"].tolist()\n",
    "text_batches = [texts[i : i + 100] for i in range(0, len(texts), 100)]\n",
    "\n",
    "# run the model\n",
    "results = []\n",
    "for batch in tqdm(text_batches):\n",
    "    try:\n",
    "        out = model_openai.batch(batch, config={\"max_concurrency\": 100})\n",
    "        results.extend(out)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        results.extend([None] * len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MadeiraInfo(especies=[ESPECIE(nome='massar', nome_cientifico='null')], cortes=[CORTE(nome='caibro')], dimensoes=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def convert_pydantic_to_gliner_format(\n",
    "    text: str, madeira_info: MadeiraInfo  # Changed parameter name and type\n",
    ") -> List[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert wood product data to GLiNER format.\n",
    "\n",
    "    Args:\n",
    "        text (str): Original text.\n",
    "        wood_info (MadeiraInfo): Pydantic model with wood product information.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of entity dictionaries in GLiNER format.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if wood_info is None:\n",
    "        return []\n",
    "\n",
    "    # Handle all three entity types with their respective labels\n",
    "    entity_mapping = [\n",
    "        (wood_info.especies, [\"nome\", \"nome_cientifico\"], [\"especie\", \"especie_cientifica\"]),\n",
    "        (wood_info.cortes, [\"nome\"], [\"corte\"]),\n",
    "        (wood_info.dimensoes, [\"medida\"], [\"dimensao\"])\n",
    "    ]\n",
    "\n",
    "    for entity_list, fields, labels in entity_mapping:\n",
    "        if entity_list is None:\n",
    "            continue\n",
    "            \n",
    "        for entity in entity_list:\n",
    "            for field, label in zip(fields, labels):\n",
    "                value = getattr(entity, field)\n",
    "                if value:\n",
    "                    # Find all matches in the text\n",
    "                    for match in re.finditer(re.escape(value), text, re.IGNORECASE):\n",
    "                        result.append({\n",
    "                            \"start\": match.start(),\n",
    "                            \"end\": match.end(),\n",
    "                            \"text\": match.group(),\n",
    "                            \"label\": label,\n",
    "                            \"score\": 1.0\n",
    "                        })\n",
    "\n",
    "    # Sort by start position\n",
    "    result.sort(key=lambda x: x[\"start\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lf_openai = pd.DataFrame(texts, columns=[\"text\"])\n",
    "df_lf_openai[\"weak_label\"] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lf_openai.to_csv('100_lf_openai.csv', sep = ';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MadeiraInfo' object has no attribute 'medicamentos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_lf_openai[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweak_label\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_lf_openai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_pydantic_to_gliner_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweak_label\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m df_lf_openai\n",
      "File \u001b[1;32mc:\\Users\\01706565437\\Documents\\Madeira-Fraca\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\01706565437\\Documents\\Madeira-Fraca\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\01706565437\\Documents\\Madeira-Fraca\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\01706565437\\Documents\\Madeira-Fraca\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m df_lf_openai[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweak_label\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_lf_openai\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mconvert_pydantic_to_gliner_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweak_label\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m df_lf_openai\n",
      "Cell \u001b[1;32mIn[71], line 19\u001b[0m, in \u001b[0;36mconvert_pydantic_to_gliner_format\u001b[1;34m(text, lista_medicamentos)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lista_medicamentos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlista_medicamentos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedicamentos\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     lista_medicamentos\u001b[38;5;241m.\u001b[39mmedicamentos \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Iterate over each medication in the Pydantic model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\01706565437\\Documents\\Madeira-Fraca\\.venv\\lib\\site-packages\\pydantic\\main.py:989\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MadeiraInfo' object has no attribute 'medicamentos'"
     ]
    }
   ],
   "source": [
    "df_lf_openai[\"weak_label\"] = df_lf_openai.apply(\n",
    "    lambda x: convert_pydantic_to_gliner_format(x.text, x.weak_label), axis=1\n",
    ")\n",
    "\n",
    "df_lf_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Gliner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Além dos anotadores que criamos com a lista de espécies e corte de madeira, e por o problema ser bem específico não fui capaz de achar nenhum tipo de modelo pré-treinado para a tarefa de NER do domínio selecionado.<br>\n",
    "Por isso vamos utilizar os modelos zero shot do GliNER, utilizando os nossos domínios esses modelos irão tentar rotular o texto sem nenhum treinamento prévio, o que pode ajudar bastante a solução da tarefa.  \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gliner import GLiNER\n",
    "\n",
    "# model_gliner_llama = GLiNER.from_pretrained(\n",
    "#     \"knowledgator/gliner-llama-1.3B-v1.0\",\n",
    "# )\n",
    "\n",
    "# model_gliner_bi_large = GLiNER.from_pretrained(\n",
    "#     \"knowledgator/gliner-bi-large-v1.0\",\n",
    "# )\n",
    "\n",
    "# model_gliner_qwen = GLiNER.from_pretrained(\n",
    "#     \"knowledgator/gliner-qwen-1.5B-v1.0\",\n",
    "# )\n",
    "\n",
    "# model_gliner_llama = model_gliner_llama.half()\n",
    "# model_gliner_bi_large = model_gliner_bi_large.half()\n",
    "# model_gliner_qwen = model_gliner_qwen.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"madeira serrada massaranduba (manilkara huberi) 2.5 metro 2 vigas de 10x15cm e 3 tabuas de 2,5x30x200cm\")\n",
    "text = doc.text\n",
    "\n",
    "labels = [\"ESPECIE\", \"CORTE\", \"DIMENSAO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "from typing import Any\n",
    "\n",
    "def render_entity_data_from_pipeline(\n",
    "    text: str,\n",
    "    pipeline_results: list[dict[str, Any]],\n",
    "    colors: dict[str, str] = None,\n",
    "    label_key_name: str = \"entity_group\",\n",
    ") -> None:\n",
    "    # Extract entity spans (start, end, label) from pipeline results\n",
    "    entity_spans = [\n",
    "        (result[\"start\"], result[\"end\"], result[label_key_name])\n",
    "        for result in pipeline_results\n",
    "    ]\n",
    "\n",
    "    # If no colors are provided, assign default colors to each entity type\n",
    "    if colors is None:\n",
    "        entity_types = list(set(span[2] for span in entity_spans))\n",
    "        # Default colors for up to 5 entity types; cycle if more\n",
    "        default_colors = ['#FF0000', '#00FF00', '#0000FF', '#FFFF00', '#FF00FF']\n",
    "        colors = {}\n",
    "        for i, entity_type in enumerate(entity_types):\n",
    "            colors[entity_type] = default_colors[i % len(default_colors)]\n",
    "\n",
    "    # Configure displacy options with entity labels and their colors\n",
    "    displacy_options = {\n",
    "        \"ents\": list(colors.keys()),\n",
    "        \"colors\": colors  # Pass the color mapping dictionary\n",
    "    }\n",
    "\n",
    "    # Prepare data in the format required by displacy\n",
    "    displacy_data = [{\n",
    "        \"text\": text,\n",
    "        \"ents\": [\n",
    "            {\"start\": start, \"end\": end, \"label\": label}\n",
    "            for start, end, label in entity_spans\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    # Render the entities using displacy\n",
    "    displacy.render(\n",
    "        displacy_data,\n",
    "        style=\"ent\",\n",
    "        manual=True,\n",
    "        jupyter=True,\n",
    "        options=displacy_options\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = model_gliner_llama.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# print(result)\n",
    "\n",
    "# render_entity_data_from_pipeline(text, result, label_key_name=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = model_gliner_bi_large.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# print(result)\n",
    "\n",
    "# render_entity_data_from_pipeline(text, result, label_key_name=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = model_gliner_qwen.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# print(result)\n",
    "\n",
    "# render_entity_data_from_pipeline(text, result, label_key_name=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Okay não começamos bem. Para o nosso exemplo simples apenas o modelo do Qwen foi capaz de identicar algumas entidades, parece que a entidade de corte foi um desafio para esses modelos.<br>\n",
    "Vamos observar em uma quantidade maior de dados, vamos utilizar 50 instâncias do conjunto de treinamento para ver se podemos utilizar esses modelos como anotadores para o nosso problema.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_test = [doc.text for doc in spacy_docs_train[:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def create_entity_dataframe(texts, model, labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Process multiple texts and create a DataFrame with entities\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts to process\n",
    "        model: Your GLiNER model\n",
    "        labels (list): Entity labels to predict\n",
    "        threshold (float): Confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with text, entities, and entity count\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for text in tqdm(texts):  # tqdm adds progress bar\n",
    "        # Get predictions from model\n",
    "        entities = model.predict_entities(text, labels, threshold=threshold)\n",
    "        \n",
    "        # Format results\n",
    "        data.append({\n",
    "            \"text\": text,\n",
    "            \"entities\": entities,\n",
    "            \"n_entities\": len(entities)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_df = create_entity_dataframe(\n",
    "#     texts=texts_to_test,\n",
    "#     model=model_gliner_llama,\n",
    "#     labels=labels,\n",
    "#     threshold=0.5\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Contando a quantidade de entidades que o Llama foi capaz de reconhecer.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_df.n_entities.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "O llama não foi capaz de achar entidades na amostra de 50 instâncias do conjunto de dados não rotulados, portanto já podemos eliminar o uso desse modelo da nossa pipeline.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi_large_df = create_entity_dataframe(\n",
    "#     texts=texts_to_test,\n",
    "#     model=model_gliner_bi_large,\n",
    "#     labels=labels,\n",
    "#     threshold=0.5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi_large_df.n_entities.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi_large_entities = bi_large_df[bi_large_df.n_entities > 0]\n",
    "# bi_large_entities[['text', 'entities']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "O modelo bi_large foi capaz de achar 3 entidades, porém todas com a entidade errada, também podemos remover esse modelo da pipeline.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen_df = create_entity_dataframe(\n",
    "#     texts=texts_to_test,\n",
    "#     model=model_gliner_qwen,\n",
    "#     labels=labels,\n",
    "#     threshold=0.5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen_df.n_entities.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen_entities = qwen_df[qwen_df.n_entities > 0]\n",
    "# qwen_entities[['text', 'entities']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "O qwen foi o melhor dos 3 modelos, além de ter achado a maior quantidade de entidades grande parte delas está correta, porém existem alguns erros preocupantes, o texto da linha 12 foi completamente selecionada, realmente existe um CORTE, na descrição mas ele não foi devidamente selecionado, além disso Muiracatiara na linha 17 foi selecionado como corte, mas deveria ser uma espécie.<br>\n",
    "Vamos observar os casos em que o modelo não selecionou nenhuma entidade para analisar se estamos tendo um bom coverage ao menos do rótulo desses textos.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen_not_entities = qwen_df[qwen_df.n_entities == 0]\n",
    "# qwen_not_entities[['text', 'entities']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "No conjunto de texto em que o Qwen não foi capaz de identificar nenhuma das entidades existem exemplos como a linha 8, 9, 22 que realmente não deveriam ser rotulados, porém a grande maioria deveria.<br>\n",
    "Por esse motivo e o grande custo que esses modelos possuem comparados as funções de rotulagem que criamos antes também não irei utilizar o Zero shot do Qwen como uma função rotuladora.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aplicando Funções de Agregação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_docs_train = list(combined.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Salvar o objeto do Spacy para não precisar rodar o processo anterior múltiplas vezes.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to artefatos/spacy_docs_train_annotated.bin...done\n"
     ]
    }
   ],
   "source": [
    "skweak.utils.docbin_writer(\n",
    "    spacy_docs_train, \"artefatos/spacy_docs_train_annotated.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skweak\n",
    "\n",
    "# Now we can load them back, so we don't need to run the labelling functions again\n",
    "spacy_docs_train = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_train_annotated.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "spacy_docs_train = list(spacy_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_voter = skweak.aggregation.MajorityVoter(\n",
    "    name=\"doclevel_voter\",\n",
    "    labels=[\"ESPECIE\", \"CORTE\", \"DIMENSAO\"],\n",
    "    initial_weights={\"doc_majority\": 0.0}  # Evitar a inclusão dele mesmo\n",
    ")\n",
    "\n",
    "spacy_docs_train = list(majority_voter.pipe(spacy_docs_train))\n",
    "\n",
    "doc_majority = skweak.doclevel.DocumentMajorityAnnotator(\"doc_majority\",\"doclevel_voter\")\n",
    "\n",
    "spacy_docs_train = list(doc_majority.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19968 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1 -132771.27819403             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19968 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2 -127659.25208165   +5112.02611238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19968 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3 -126179.63493980   +1479.61714186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19968 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4 -125446.25398991    +733.38094989\n"
     ]
    }
   ],
   "source": [
    "initial_weights = {\n",
    "    \"especie_annotator\": .9,\n",
    "    \"cientifico_especie_annotator\": 1,\n",
    "    \"corte_annotator\": 1,\n",
    "    \"dimensao_annotator\": 1,\n",
    "    \"llm_annotator\": 0.7,   \n",
    "    \"doclevel_voter\": 0.5,\n",
    "    \"doc_majority\": 0.6\n",
    "}\n",
    "\n",
    "hmm = skweak.generative.HMM(\n",
    "    \"hmm\", \n",
    "    labels=[\"ESPECIE\", \"CORTE\", \"DIMENSAO\"],\n",
    "    initial_weights=initial_weights\n",
    ")\n",
    "\n",
    "\n",
    "hmm.fit(spacy_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM model with following parameters:\n",
      "Output labels: ['O', 'B-ESPECIE', 'I-ESPECIE', 'B-CORTE', 'I-CORTE', 'B-DIMENSAO', 'I-DIMENSAO']\n",
      "--------\n",
      "Start distribution:\n",
      "O             0.49\n",
      "B-ESPECIE     0.16\n",
      "I-ESPECIE     0.00\n",
      "B-CORTE       0.35\n",
      "I-CORTE       0.00\n",
      "B-DIMENSAO    0.00\n",
      "I-DIMENSAO    0.00\n",
      "dtype: float64\n",
      "--------\n",
      "Transition model:\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.78       0.03       0.00     0.15      0.0        0.05        0.00\n",
      "B-ESPECIE   0.64       0.00       0.17     0.00      0.0        0.19        0.00\n",
      "I-ESPECIE   0.86       0.00       0.00     0.01      0.0        0.14        0.00\n",
      "B-CORTE     0.67       0.21       0.00     0.01      0.0        0.12        0.00\n",
      "I-CORTE     0.20       0.20       0.00     0.20      0.2        0.20        0.00\n",
      "B-DIMENSAO  0.03       0.00       0.00     0.00      0.0        0.01        0.96\n",
      "I-DIMENSAO  0.41       0.03       0.00     0.00      0.0        0.17        0.39\n",
      "--------\n",
      "Labelling functions in model: ['cientifico_especie_annotator', 'corte_annotator', 'doc_majority', 'especie_annotator', 'dimensao_annotator']\n",
      "Emission model for: cientifico_especie_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.90       0.06       0.04     0.00     0.00        0.00        0.00\n",
      "B-ESPECIE   0.01       0.99       0.00     0.00     0.00        0.00        0.00\n",
      "I-ESPECIE   0.00       0.00       1.00     0.00     0.00        0.00        0.00\n",
      "B-CORTE     1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-CORTE     0.23       0.00       0.00     0.00     0.77        0.00        0.00\n",
      "B-DIMENSAO  1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-DIMENSAO  0.99       0.01       0.00     0.00     0.00        0.00        0.00\n",
      "weights     0.95       0.96       1.00     0.93     0.93        0.93        0.93\n",
      "--------\n",
      "Emission model for: corte_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.99        0.0        0.0     0.01     0.00         0.0         0.0\n",
      "B-ESPECIE   1.00        0.0        0.0     0.00     0.00         0.0         0.0\n",
      "I-ESPECIE   1.00        0.0        0.0     0.00     0.00         0.0         0.0\n",
      "B-CORTE     0.00        0.0        0.0     1.00     0.00         0.0         0.0\n",
      "I-CORTE     0.23        0.0        0.0     0.00     0.77         0.0         0.0\n",
      "B-DIMENSAO  1.00        0.0        0.0     0.00     0.00         0.0         0.0\n",
      "I-DIMENSAO  0.99        0.0        0.0     0.01     0.00         0.0         0.0\n",
      "weights     1.00        1.0        1.0     1.00     1.00         1.0         1.0\n",
      "--------\n",
      "Emission model for: dimensao_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.91        0.0        0.0      0.0     0.00        0.09         0.0\n",
      "B-ESPECIE   1.00        0.0        0.0      0.0     0.00        0.00         0.0\n",
      "I-ESPECIE   1.00        0.0        0.0      0.0     0.00        0.00         0.0\n",
      "B-CORTE     1.00        0.0        0.0      0.0     0.00        0.00         0.0\n",
      "I-CORTE     0.23        0.0        0.0      0.0     0.77        0.00         0.0\n",
      "B-DIMENSAO  0.00        0.0        0.0      0.0     0.00        1.00         0.0\n",
      "I-DIMENSAO  0.00        0.0        0.0      0.0     0.00        0.00         1.0\n",
      "weights     1.00        1.0        1.0      1.0     1.00        1.00         1.0\n",
      "--------\n",
      "Emission model for: doc_majority\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.93       0.06       0.00     0.00     0.00        0.00        0.00\n",
      "B-ESPECIE   0.20       0.80       0.00     0.00     0.00        0.00        0.00\n",
      "I-ESPECIE   0.05       0.58       0.36     0.00     0.00        0.00        0.00\n",
      "B-CORTE     0.53       0.00       0.00     0.47     0.00        0.00        0.00\n",
      "I-CORTE     0.23       0.00       0.00     0.00     0.77        0.00        0.00\n",
      "B-DIMENSAO  0.76       0.00       0.00     0.00     0.00        0.24        0.00\n",
      "I-DIMENSAO  0.71       0.14       0.00     0.00     0.00        0.00        0.15\n",
      "weights     0.60       0.60       0.60     0.60     0.60        0.60        0.60\n",
      "--------\n",
      "Emission model for: especie_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.90       0.10       0.00     0.00     0.00        0.00        0.00\n",
      "B-ESPECIE   0.00       1.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-ESPECIE   0.03       0.00       0.97     0.00     0.00        0.00        0.00\n",
      "B-CORTE     1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-CORTE     0.23       0.00       0.00     0.00     0.77        0.00        0.00\n",
      "B-DIMENSAO  1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-DIMENSAO  0.92       0.08       0.00     0.00     0.00        0.00        0.00\n",
      "weights     0.85       0.87       0.90     0.83     0.83        0.83        0.83\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "hmm.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_voter = skweak.voting.SequentialMajorityVoter(\n",
    "    \"maj_voter\",\n",
    "    labels=[\"ESPECIE\", \"CORTE\", \"DIMENSAO\"],\n",
    "    initial_weights=initial_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_docs_train = list(hmm.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_docs_train = list(maj_voter.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HMM Annotations ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7FFFD4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    angelim verm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " aproveitamento \n",
       "<mark class=\"entity\" style=\"background: #FFD700; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ripas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Majority Voter Annotations ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7FFFD4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    angelim verm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " aproveitamento \n",
       "<mark class=\"entity\" style=\"background: #FFD700; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ripas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render_skweak_entities(doc, annotator_name: str, colors: dict = None):\n",
    "    \"\"\"Render entities from skweak annotations with colored highlighting\"\"\"\n",
    "    \n",
    "    # Get entities from specified annotator\n",
    "    entities = doc.spans.get(annotator_name, [])\n",
    "    \n",
    "    # Default color scheme for your entities\n",
    "    default_colors = {\n",
    "        \"ESPECIE\": \"#7FFFD4\",  # Aquamarine\n",
    "        \"CORTE\": \"#FFD700\",    # Gold\n",
    "        \"DIMENSAO\": \"#FFB6C1\"  # Pink\n",
    "    }\n",
    "    \n",
    "    # Ensure colors is never None\n",
    "    colors = colors or default_colors\n",
    "    \n",
    "    # Prepare entities for visualization\n",
    "    displacy_data = {\n",
    "        \"text\": doc.text,\n",
    "        \"ents\": [\n",
    "            {\n",
    "                \"start\": ent.start_char,\n",
    "                \"end\": ent.end_char,\n",
    "                \"label\": ent.label_\n",
    "            } for ent in entities\n",
    "        ],\n",
    "        \"title\": None\n",
    "    }\n",
    "\n",
    "    # Render with specified colors (ensure empty dict if no colors)\n",
    "    displacy.render(\n",
    "        [displacy_data],\n",
    "        style=\"ent\",\n",
    "        manual=True,\n",
    "        jupyter=True,\n",
    "        options={\"colors\": colors or {}}  # Key fix here\n",
    "    )\n",
    "\n",
    "# Usage\n",
    "doc = spacy_docs_train[0]\n",
    "\n",
    "print(\"=== HMM Annotations ===\")\n",
    "render_skweak_entities(doc, \"hmm\")\n",
    "\n",
    "\n",
    "print(\"=== Majority Voter Annotations ===\")\n",
    "render_skweak_entities(doc, \"maj_voter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Se\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "docs_sem_entidades = [\n",
    "    doc for doc in spacy_docs_train[:50] \n",
    "    if len(doc.spans.get(\"hmm\", [])) == 0\n",
    "]\n",
    "\n",
    "df_sem_entidades_hmm = pd.DataFrame({\n",
    "    \"text\": [doc.text for doc in docs_sem_entidades],\n",
    "    \"num_entidades\": [len(doc.spans.get(\"hmm\", [])) for doc in docs_sem_entidades]\n",
    "})\n",
    "\n",
    "df_sem_entidades_maj_voter = pd.DataFrame({\n",
    "    \"text\": [doc.text for doc in docs_sem_entidades],\n",
    "    \"num_entidades\": [len(doc.spans.get(\"maj_voter\", [])) for doc in docs_sem_entidades]\n",
    "})\n",
    "\n",
    "print(len(df_sem_entidades_hmm))\n",
    "print(len(df_sem_entidades_maj_voter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_entidades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jogo cozinha em madeira</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conjunto de utensilios para cozi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>palitos p/ dentes aurea gde</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pv-kit colher / espatula em made</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text  num_entidades\n",
       "0           jogo cozinha em madeira              0\n",
       "1  conjunto de utensilios para cozi              0\n",
       "2       palitos p/ dentes aurea gde              0\n",
       "3  pv-kit colher / espatula em made              0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sem_entidades_hmm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_entidades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jogo cozinha em madeira</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conjunto de utensilios para cozi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>palitos p/ dentes aurea gde</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pv-kit colher / espatula em made</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               text  num_entidades\n",
       "0           jogo cozinha em madeira              0\n",
       "1  conjunto de utensilios para cozi              0\n",
       "2       palitos p/ dentes aurea gde              0\n",
       "3  pv-kit colher / espatula em made              0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sem_entidades_maj_voter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Parece que ambos agregadores conseguiram explorar bem o conjunto de dados e identificar ao menos uma entidade quando o produto era realmente de madeira serrada.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to artefatos/spacy_docs_train_annotated.bin...done\n"
     ]
    }
   ],
   "source": [
    "from skweak.utils import docbin_writer\n",
    "docbin_writer(spacy_docs_train, \"artefatos/spacy_docs_train_annotated.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utilizar as Weak Labels para produzir um novo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Agora preciso utilizar as funções de rotulagem que implementei até agora para fazer o fine-tuning de um modelo Bert pré-treinado capaz de generalizar as regras 'hard' do Gazetteer e da heurística que criei, além das funções agregadoras.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skweak.utils import docbin_reader\n",
    "\n",
    "spacy_docs_train = list(docbin_reader(\"artefatos/spacy_docs_train_annotated.bin\", \n",
    "                                     spacy_model_name=\"pt_core_news_lg\"))\n",
    "\n",
    "doc = spacy_docs_train[0]\n",
    "print(\"Entidades:\", [(ent.text, ent.label_) for ent in doc.spans[\"hmm\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_iob(docs, annotator_name=\"hmm\"):\n",
    "    \"\"\"Convert skweak annotations to IOB format\"\"\"\n",
    "    iob_data = []\n",
    "    for doc in docs:\n",
    "        tokens = [token.text for token in doc]\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "        \n",
    "        # Process each entity span\n",
    "        for span in doc.spans[annotator_name]:\n",
    "            label = span.label_\n",
    "            start = span.start\n",
    "            end = span.end\n",
    "            \n",
    "            labels[start] = f\"B-{label}\"\n",
    "            for i in range(start+1, end):\n",
    "                labels[i] = f\"I-{label}\"\n",
    "                \n",
    "        iob_data.append({\"tokens\": tokens, \"labels\": labels, \"text\": doc.text})\n",
    "    \n",
    "    return iob_data\n",
    "\n",
    "train_data = convert_to_iob(spacy_docs_train, annotator_name=\"hmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_argilla_to_iob(argilla_data):\n",
    "    \"\"\"Convert Argilla format to IOB format\"\"\"\n",
    "    iob_data = []\n",
    "    \n",
    "    for example in argilla_data:\n",
    "        text = example[\"text\"]\n",
    "        entities = example.get(\"span_label.responses\", [])\n",
    "        \n",
    "        if isinstance(entities, str):\n",
    "            try:\n",
    "                clean_str = entities.replace(\"array(\", \"\").replace(\", dtype=object)\", \"\")\n",
    "                entities = ast.literal_eval(clean_str)\n",
    "            except:\n",
    "                entities = []\n",
    "\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "\n",
    "        for entity in entities:\n",
    "            if not isinstance(entity, dict):\n",
    "                continue\n",
    "\n",
    "            start = entity[\"start\"]\n",
    "            end = entity[\"end\"]\n",
    "            label = entity[\"label\"]\n",
    "\n",
    "            start_token = None\n",
    "            for token in doc:\n",
    "                if token.idx >= start:\n",
    "                    break\n",
    "                start_token = token.i\n",
    "            if start_token is None:\n",
    "                start_token = 0\n",
    "\n",
    "            end_token = None\n",
    "            for token in doc:\n",
    "                if token.idx + len(token) > end:\n",
    "                    break\n",
    "                end_token = token.i\n",
    "            if end_token is None:\n",
    "                end_token = len(doc) - 1\n",
    "\n",
    "            if start_token <= end_token:\n",
    "                labels[start_token] = f\"B-{label}\"\n",
    "                for i in range(start_token + 1, end_token + 1):\n",
    "                    labels[i] = f\"I-{label}\"\n",
    "\n",
    "        iob_data.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"labels\": labels,\n",
    "            \"text\": text\n",
    "        })\n",
    "    \n",
    "    return iob_data\n",
    "\n",
    "val_data = convert_argilla_to_iob(df_val.to_dict(\"records\"))\n",
    "test_data = convert_argilla_to_iob(df_test.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Sequence, ClassLabel, Value\n",
    "\n",
    "labels = [\"O\", \"B-ESPECIE\", \"I-ESPECIE\", \"B-CORTE\", \"I-CORTE\", \"B-DIMENSAO\", \"I-DIMENSAO\"]\n",
    "\n",
    "dataset_train = Dataset.from_list(train_data).cast(Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    \"text\": Value(\"string\")\n",
    "}))\n",
    "dataset_val = Dataset.from_list(val_data).cast(Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    \"text\": Value(\"string\")\n",
    "}))\n",
    "dataset_test = Dataset.from_list(test_data).cast(Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    \"text\": Value(\"string\")\n",
    "}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset_train = dataset_train.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_dataset_val = dataset_val.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_dataset_test = dataset_test.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, max_length=512, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"neuralmind/bert-base-portuguese-cased\",\n",
    "    num_labels=len(labels),\n",
    "    id2label={i: label for i, label in enumerate(labels)},\n",
    "    label2id={label: i for i, label in enumerate(labels)}\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./artefatos/bert-base-ner-true-labels\",  # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    per_device_train_batch_size=128,  # Batch size for training\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=1,  # Accumulate gradients over multiple steps\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    "    bf16=True,  # Use bfloat16 precision for training (if supported by hardware)\n",
    "    fp16=False,  # Use mixed precision training with FP16 (if supported by hardware)\n",
    "    save_total_limit=1,  # Limit the total number of saved checkpoints\n",
    "    logging_steps=1,  # Log training metrics every step\n",
    "    eval_steps=1,  # Evaluate the model every step\n",
    "    save_steps=1,  # Save the model every step\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to determine the best model\n",
    "    greater_is_better=False,  # Higher metric value is better\n",
    "    logging_strategy=\"steps\",  # Log metrics at each step\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    do_train=True,  # Perform training\n",
    "    do_eval=True,  # Perform evaluation\n",
    "    push_to_hub=False,  # Do not push the model to the Hugging Face Hub\n",
    "    learning_rate=1e-5,  # Learning rate for the optimizer\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "Parece que o modelo não foi capaz de generalizar. Mesmo após fazer alguns testes modificando os parâmteros de treino, como o learning rate, per_device_train_batch_size, weight_decay, eu não fui capaz de encontrar uma combinação que diminuísse a Validation Loss.<br>\n",
    "Mesmo assim vamos analizar o comportamento desse modelo com alguns exemplos e no nosso conjunto de teste.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"angelim verm aproveitamento ripas\" ## Exemplo do conjunto de treino, errou em ri ## pas\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "print(\"Entities detected:\")\n",
    "for entity in results:\n",
    "    print(f\"{entity['word']} -> {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_teste = tokenized_dataset_test['text'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = ner_pipeline(textos_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entities detected:\")\n",
    "for entity in results_list[0]:\n",
    "    print(f\"{entity['word']} -> {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entities detected:\")\n",
    "for entity in results_list[1]:\n",
    "    print(f\"{entity['word']} -> {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytics & Market Intelligence\n",
    "\n",
    "Use case: Aggregate and analyze trends across product descriptions.\n",
    "\n",
    "Why it matters:\n",
    "\n",
    "    What species are trending?\n",
    "\n",
    "    Is there increased demand for rift-sawn oak in certain dimensions?\n",
    "\n",
    "    Your model enables text mining on raw product text to derive insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured Product Catalogs for E-commerce Platforms\n",
    "\n",
    "Use case: Automatically structuring unstructured product descriptions into standardized fields.\n",
    "\n",
    "Why it matters:\n",
    "\n",
    "    Timber suppliers often have messy, inconsistent listings (e.g., “Oak rift sawn board 2x6” vs. “2x6 white oak, rift cut”).\n",
    "\n",
    "    Your NER model can power a data normalization pipeline that extracts:\n",
    "\n",
    "        Species: white oak, Douglas fir\n",
    "\n",
    "        Cut type: plain sawn, quarter sawn, rift sawn\n",
    "\n",
    "        Dimensions: 2x6, 25mm x 200mm\n",
    "\n",
    "Outcome: Cleaner databases, improved search/filter/sorting, better inventory management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
