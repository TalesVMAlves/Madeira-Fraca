{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como informado em sala de aula, a nota da unidade 1 deverá ser enviada indivudalmente aqui e apresentada no dia 08/05/2025.\n",
    "\n",
    "A atividade 1 consistirá no envio do link de uma pasta compartilhada no google drive ou um repositório no github contendo:\n",
    "\n",
    "1 - Datasets e todos os artefatos utilizados\n",
    "\n",
    "2 - Cadernos jupyter com o processo aplicado em uma pipeline de supervisão fraca. \n",
    "\n",
    "O caderno deverá primar pela qualidade, incluindo:\n",
    "\n",
    "a) Análise exploratória de dados\n",
    "\n",
    "b) Storytelling\n",
    "\n",
    "c) Explicação detalhada de cada etapa\n",
    "\n",
    " \n",
    "\n",
    "As células devem estar EXECUTADAS, com a saída registrada no caderno.\n",
    "\n",
    " \n",
    "\n",
    " A atividade será avaliada pela qualidade técnica e esmero na construção do caderno.\n",
    "\n",
    " \n",
    "\n",
    "Um abraço,\n",
    "\n",
    " \n",
    "\n",
    "Elias \n",
    "\n",
    "Obs: Certifique-se de que a pasta/repositório do Github esteja acessível para qualquer pessoa com um link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_val_test = pd.read_csv('dataset/labeledTokenClassification.csv')\n",
    "df_unlabeled = pd.read_csv('dataset/unlabeledMadeiraDescricao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_test = df_val_test[df_val_test.status == 'completed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_val, df_test = train_test_split(\n",
    "    df_val_test,\n",
    "    test_size=0.5,       \n",
    "    random_state=1, # Necessário a mudança caso utilizar o conjunto de teste\n",
    "    shuffle=True         \n",
    ")\n",
    "print(len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21010"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeiro Passo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skweak\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to artefatos/spacy_docs_train.bin...done\n",
      "Write to artefatos/spacy_docs_dev.bin...done\n",
      "Write to artefatos/spacy_docs_test.bin...done\n"
     ]
    }
   ],
   "source": [
    "spacy_docs_train = list(nlp.pipe(df_unlabeled[\"text\"].values))\n",
    "\n",
    "spacy_docs_dev = list(nlp.pipe(df_val[\"text\"].values))\n",
    "\n",
    "spacy_docs_test = list(nlp.pipe(df_test[\"text\"].values))\n",
    "\n",
    "skweak.utils.docbin_writer(spacy_docs_train, \"artefatos/spacy_docs_train.bin\")\n",
    "skweak.utils.docbin_writer(spacy_docs_dev, \"artefatos/spacy_docs_dev.bin\")\n",
    "skweak.utils.docbin_writer(spacy_docs_test, \"artefatos/spacy_docs_test.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training documents\n",
    "spacy_docs_train = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_train.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "\n",
    "# Load the validation documents\n",
    "spacy_docs_dev = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_dev.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "\n",
    "# Load the test documents\n",
    "spacy_docs_test = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_dev.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "\n",
    "# Convert the loaded documents to lists\n",
    "# This step ensures that the documents are in a list format, which is easier to work with in subsequent steps\n",
    "spacy_docs_train = list(spacy_docs_train)\n",
    "spacy_docs_dev = list(spacy_docs_dev)\n",
    "spacy_docs_test = list(spacy_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[angelim verm aproveitamento ripas, caibro mixto 4,5mt]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_docs_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionar o nom cientifico das especies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "madeiras_dicionario = {\n",
    "    'acacia': ['angico'],\n",
    "    'acapu': ['angelim de folha larga'],\n",
    "    'amapa': [],\n",
    "    'amescla': ['amesclao', 'breu', 'morcegueira', 'sucuruba'],\n",
    "    'andiroba': ['angirova', 'carapa'],\n",
    "    'angelim amargoso': ['angelim amarelo', 'fava', 'fava amargosa', 'faveira'],\n",
    "    'angelim pedra': ['angelim verm', 'angelim vermelho', 'angelim falso', 'faveira ferro'],\n",
    "    'angelim indefinido': ['angelim', 'angelin', 'ang'],\n",
    "    'cedrinho': ['quaruba', 'quaruvatinga', 'mandioqueira', 'quaruba vermelha'],\n",
    "    'cedro': [],\n",
    "    'cerejeira': [],\n",
    "    'cetim': ['pau amarelo', 'pau cetim', 'piquia cetim', 'garapa', ' muirajuba'],\n",
    "    'coco pau': ['baru'],\n",
    "    'copaiba': ['cupauba', 'copauva'],\n",
    "    'cumaru': ['cumaru de folha grande', 'cumbar', 'cumbaru roxo'],\n",
    "    'cupiuba': ['copiuba', 'copiuva'],\n",
    "    'eucalipto': ['euc'],\n",
    "    'freijo': [],\n",
    "    'garapeira': [],\n",
    "    'goiabao': ['abiu', 'abiurana'],\n",
    "    'guajara': ['currupixa', 'angico-bravo', 'angico-cedro', 'angico-rosa', 'curupia'],\n",
    "    'ipe': [],\n",
    "    'itauba': [],\n",
    "    'jatoba': ['jatai', 'utai', 'jati'],\n",
    "    'jequitiba': ['cachimbeiro', 'estopeira', 'coatinga', 'bingueiro'],\n",
    "    'louro amarelo': ['louro pardo, louro abacate', 'louro inhamui'],\n",
    "    'louro canela': ['louro rosa', 'louro gamela', 'louro vermelho'],\n",
    "    'louro indefinido': ['louro'],\n",
    "    'louro preto': [],\n",
    "    'marupa': [],\n",
    "    'massaranduba': ['macaranduba', 'masaranduba', 'maparajuba', 'aparaui', 'paraju', 'mass'],\n",
    "    'matamata': [],\n",
    "    'melanceira': ['sucupira-pepino'],\n",
    "    'mogno': [],\n",
    "    'muiracatiara': [],\n",
    "    'para para': ['morototo', 'mandioqueira'],\n",
    "    'pau louro': [],\n",
    "    'pau mulato': [],\n",
    "    'peroba': ['amapa amargoso', 'parahancornia'],\n",
    "    'pinho': ['araucaria'],\n",
    "    'pinus': ['pinheiro', 'pinnus'],\n",
    "    'piquia': ['piqui', 'piquiarana', 'pequia'],\n",
    "    'roxinho': ['pau roxo', 'guarabu', 'pau ferro'],\n",
    "    'sapucaia': ['jarana'],\n",
    "    'sucupira': ['cutiba', 'sapupira', 'sebepira'],\n",
    "    'sumauma': ['ceiba'],\n",
    "    'tanimbuca': ['amarelao', 'mirindiba', 'embiridiba', 'cuiarana ,carana', 'mangue'],\n",
    "    'tatajuba': ['amarelo', 'amarelinho', 'garrote', 'bagaceira'],\n",
    "    'tauari': ['imbirema', 'estopeiro', 'toari'],\n",
    "    'taxi': [],\n",
    "    'timborana': ['timbauba', 'angico'],\n",
    "    'uxi': ['axua', 'cumate', 'paruru'],\n",
    "    'mista': ['misto', 'mixta', 'mix'],\n",
    "    'virola': ['ucuuba']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cientifico_madeiras_dicionario = {\n",
    "    'acapu': ['vouacapoua americana', 'vouacapoua'],\n",
    "    'amapa': ['parahancornia amapa', 'parahancornia'],\n",
    "    'amescla': ['protium heptaphyllum', 'protium', 'heptaphyllum'],\n",
    "    'andiroba': ['carapa guianensis', 'carapa', 'guianensis'],\n",
    "    'angelim amargoso': ['vatairea sericea', 'vatairea', 'sericea'],\n",
    "    'angelim pedra': ['hymenolobium petraeum', 'hymenolobium', 'petraeum'],\n",
    "    'angelim indefinido': ['dinizia excelsa', 'dinizia', 'excelsa'],\n",
    "    'cedrinho': ['erisma uncinatum', 'erisma', 'uncinatum'],\n",
    "    'cedro': ['cedrela fissilis', 'cedrela', 'fissilis'],\n",
    "    'cerejeira': ['amburana cearensis', 'amburana', 'cearensis'],\n",
    "    'cetim': ['cedrelinga cateniformis', 'cedrelinga', 'cateniformis'],\n",
    "    'coco pau': ['orbygnia phalerata', 'orbygnia', 'phalerata'],\n",
    "    'copaiba': ['copaifera langsdorffii', 'copaifera', 'langsdorffii'],\n",
    "    'cumaru': ['dipteryx odorata', 'dipteryx', 'odorata'],\n",
    "    'cupiuba': ['goupia glabra', 'goupia', 'glabra'],\n",
    "    'eucalipto': ['eucalyptus'],\n",
    "    'freijo': ['cordia goeldiana', 'cordia', 'goeldiana'],\n",
    "    'garapeira': ['apuleia leiocarpa', 'apuleia', 'leiocarpa'],\n",
    "    'goiabao': ['qualea'],\n",
    "    'guajara': ['terminalia amazonia', 'terminalia', 'amazonia'],\n",
    "    'ipe': ['handroanthus'],\n",
    "    'itauba': ['mezilaurus itauba', 'mezilaurus', 'itauba'],\n",
    "    'jatoba': ['hymenaea courbaril', 'hymenaea', 'courbaril'],\n",
    "    'jequitiba': ['cariniana legalis', 'cariniana', 'legalis'],\n",
    "    'louro amarelo': ['ocotea'],\n",
    "    'louro canela': ['nectandra'],\n",
    "    'louro indefinido': ['ocotea'],\n",
    "    'louro preto': ['ocotea porosa', 'ocotea', 'porosa'],\n",
    "    'marupa': ['simarouba amara', 'simarouba', 'amara'],\n",
    "    'massaranduba': ['manilkara huberi', 'manilkara', 'huberi'],\n",
    "    'matamata': ['eschweilera'],\n",
    "    'melanceira': ['trattinnickia burserifolia', 'trattinnickia', 'burserifolia'],\n",
    "    'mogno': ['swietenia macrophylla', 'swietenia', 'macrophylla'],\n",
    "    'muiracatiara': ['astronium lecointei', 'astronium', 'lecointei'],\n",
    "    'para para': ['jacaranda copaia', 'jacaranda', 'copaia'],\n",
    "    'pau louro': ['ocotea'],\n",
    "    'pau mulato': ['calycophyllum spruceanum', 'calycophyllum', 'spruceanum'],\n",
    "    'peroba': ['aspidosperma'],\n",
    "    'pinho': ['araucaria angustifolia', 'araucaria', 'angustifolia'],\n",
    "    'pinus': ['pinus'],\n",
    "    'piquia': ['caryocar villosum', 'caryocar', 'villosum'],\n",
    "    'roxinho': ['peltogyne'],\n",
    "    'sapucaia': ['lecythis pisonis', 'lecythis', 'pisonis'],\n",
    "    'sucupira': ['bowdichia virgilioides', 'bowdichia', 'virgilioides'],\n",
    "    'sumauma': ['ceiba pentandra', 'ceiba', 'pentandra'],\n",
    "    'tanimbuca': ['buchenavia tetraphylla', 'buchenavia', 'tetraphylla'],\n",
    "    'tatajuba': ['bagassa guianensis', 'bagassa', 'guianensis'],\n",
    "    'tauari': ['couratari'],\n",
    "    'taxi': ['sclerolobium'],\n",
    "    'timborana': ['enterolobium schomburgkii', 'enterolobium', 'schomburgkii'],\n",
    "    'uxi': ['endopleura uchi', 'endopleura', 'uchi'],\n",
    "    'virola': ['virola']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "apresentacoes_dicionario ={\n",
    "                            'alizar': [],\n",
    "                            'barrote':[],\n",
    "                            'caibro':['caibrinho'],\n",
    "                            'frechal': [],\n",
    "                            'janela': ['janelao'],\n",
    "                            'linha':[],\n",
    "                            'mourao': [],\n",
    "                            'quadrado': ['bloco', 'file', 'quadradinho'],\n",
    "                            'piquete': [],\n",
    "                            'pontalete': [],\n",
    "                            'porta': ['portal', 'aduela'],\n",
    "                            'prancha':['pranchoes', 'pranchinha'],\n",
    "                            'ripa':['ripao'],\n",
    "                            'tabua':[],\n",
    "                            'taipa':[],\n",
    "                            'sarrafo':['serrafo'],\n",
    "                            'viga':['virga','vigota', 'poste']\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adc_plural(apresentacoes_dict):\n",
    "    updated_dict = {}\n",
    "    for key, synonyms in apresentacoes_dict.items():\n",
    "        pluralized_synonyms = []\n",
    "        \n",
    "        if key[-1].lower() in{'a', 'e', 'i', 'o', 'u'}:\n",
    "            plural = key + \"s\"\n",
    "        \n",
    "            if plural not in synonyms:\n",
    "                pluralized_synonyms.append(plural)\n",
    "\n",
    "        for term in synonyms:\n",
    "            pluralized_synonyms.append(term)\n",
    "            \n",
    "            if term and term[-1].lower() in {'a', 'e', 'i', 'o', 'u'}:\n",
    "                if \" \" in term:\n",
    "                    parts = term.split()\n",
    "                    parts[-1] += \"s\"\n",
    "                    plural = \" \".join(parts)\n",
    "                else:\n",
    "                    plural = term + \"s\"\n",
    "                pluralized_synonyms.append(plural)\n",
    "        \n",
    "        seen = set()\n",
    "        unique_synonyms = [t for t in pluralized_synonyms \n",
    "                          if not (t in seen or seen.add(t))]\n",
    "        \n",
    "        updated_dict[key] = unique_synonyms\n",
    "        \n",
    "    return updated_dict\n",
    "\n",
    "apresentacoes_dicionario = adc_plural(apresentacoes_dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "## (PALAVRA) pode conter uma palavra | PALAVRA precisa conter uma palavra.\n",
    "padroes = [\n",
    "    # NUM X NUM X NUM (PALAVRA) -- 2.5 x 3 x 4 cm\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]*)', \n",
    "    # NUM PALAVRA X NUM PALAVRA X NUM PALAVRA -- 2.5 cm x 3 m x 4 mm\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\b', \n",
    "    # NUM X NUM (PALAVRA) -- 2 x 3 cm\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]*)', \n",
    "    # NUM PALAVRA X NUM PALAVRA -- 2cm x 3m\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\s*x\\s*(\\d+[\\.,]?\\d*)\\s*([a-z]+)\\b',\n",
    "    # NUM X NUM -- 2.5 x 3\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*x\\s*(\\d+[\\.,]?\\d*)\\b',\n",
    "    # NUM (m, cm, mm, metro, pol)\n",
    "    r'\\b(\\d+[\\.,]?\\d*)\\s*(m|cm|mm|metro|metros|pol)\\b'\n",
    "]\n",
    "\n",
    "padroes_compilados = [re.compile(padrao, re.IGNORECASE) for padrao in padroes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_dimensao(doc):\n",
    "    text = doc.text\n",
    "    for padrao in padroes_compilados:\n",
    "        for match in padrao.finditer(text):\n",
    "            start_char, end_char = match.span()\n",
    "            span = doc.char_span(start_char, end_char, alignment_mode=\"expand\")\n",
    "            if span:\n",
    "                yield span.start, span.end, \"DIMENSAO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skweak import heuristics\n",
    "dimensao_annotator = heuristics.FunctionAnnotator(\n",
    "    \"dimensao_annotator\", \n",
    "    detectar_dimensao,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skweak.gazetteers import GazetteerAnnotator, Trie\n",
    "from skweak.base import CombinedAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "especie_terms = []\n",
    "for key, synonyms in madeiras_dicionario.items():\n",
    "    especie_terms.append(key)\n",
    "    especie_terms.extend(synonyms)\n",
    "\n",
    "cientifico_especie_terms = []\n",
    "for key, cientifico in cientifico_madeiras_dicionario.items():\n",
    "    cientifico_especie_terms.extend(cientifico)\n",
    "\n",
    "corte_terms = []\n",
    "for key, synonyms in apresentacoes_dicionario.items():\n",
    "    corte_terms.append(key)\n",
    "    corte_terms.extend(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trie(term_list):\n",
    "    \"\"\"Convert a list of terms into a trie of token sequences\"\"\"\n",
    "    trie = Trie()\n",
    "    for term in term_list:\n",
    "        tokens = term.split()\n",
    "        trie.add(tokens)\n",
    "    return trie\n",
    "\n",
    "especie_trie = build_trie(especie_terms)\n",
    "cientifico_especie_trie = build_trie(cientifico_especie_terms)\n",
    "corte_trie = build_trie(corte_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['vouacapoua'], ['vouacapoua', 'americana'], ['parahancornia'], ['parahancornia', 'amapa'], ['protium'], ['protium', 'heptaphyllum'], ['heptaphyllum'], ['carapa'], ['carapa', 'guianensis'], ['guianensis'], ['vatairea'], ['vatairea', 'sericea'], ['sericea'], ['hymenolobium'], ['hymenolobium', 'petraeum'], ['petraeum'], ['dinizia'], ['dinizia', 'excelsa'], ['excelsa'], ['erisma'], ['erisma', 'uncinatum'], ['uncinatum'], ['cedrela'], ['cedrela', 'fissilis'], ['fissilis'], ['amburana'], ['amburana', 'cearensis'], ['cearensis'], ['cedrelinga'], ['cedrelinga', 'cateniformis'], ['cateniformis'], ['orbygnia'], ['orbygnia', 'phalerata'], ['phalerata'], ['copaifera'], ['copaifera', 'langsdorffii'], ['langsdorffii'], ['dipteryx'], ['dipteryx', 'odorata'], ['odorata'], ['goupia'], ['goupia', 'glabra'], ['glabra'], ['eucalyptus'], ['cordia'], ['cordia', 'goeldiana'], ['goeldiana'], ['apuleia'], ['apuleia', 'leiocarpa'], ['leiocarpa'], ['qualea'], ['terminalia'], ['terminalia', 'amazonia'], ['amazonia'], ['handroanthus'], ['mezilaurus'], ['mezilaurus', 'itauba'], ['itauba'], ['hymenaea'], ['hymenaea', 'courbaril'], ['courbaril'], ['cariniana'], ['cariniana', 'legalis'], ['legalis'], ['ocotea'], ['ocotea', 'porosa'], ['nectandra'], ['porosa'], ['simarouba'], ['simarouba', 'amara'], ['amara'], ['manilkara'], ['manilkara', 'huberi'], ['huberi'], ['eschweilera'], ['trattinnickia'], ['trattinnickia', 'burserifolia'], ['burserifolia'], ['swietenia'], ['swietenia', 'macrophylla'], ['macrophylla'], ['astronium'], ['astronium', 'lecointei'], ['lecointei'], ['jacaranda'], ['jacaranda', 'copaia'], ['copaia'], ['calycophyllum'], ['calycophyllum', 'spruceanum'], ['spruceanum'], ['aspidosperma'], ['araucaria'], ['araucaria', 'angustifolia'], ['angustifolia'], ['pinus'], ['caryocar'], ['caryocar', 'villosum'], ['villosum'], ['peltogyne'], ['lecythis'], ['lecythis', 'pisonis'], ['pisonis'], ['bowdichia'], ['bowdichia', 'virgilioides'], ['virgilioides'], ['ceiba'], ['ceiba', 'pentandra'], ['pentandra'], ['buchenavia'], ['buchenavia', 'tetraphylla'], ['tetraphylla'], ['bagassa'], ['bagassa', 'guianensis'], ['couratari'], ['sclerolobium'], ['enterolobium'], ['enterolobium', 'schomburgkii'], ['schomburgkii'], ['endopleura'], ['endopleura', 'uchi'], ['uchi'], ['virola']]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cientifico_especie_trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "especie_annotator = GazetteerAnnotator(\"especie_annotator\", {\"ESPECIE\": especie_trie})\n",
    "cientifico_especie_annotator = GazetteerAnnotator(\"cientifico_especie_annotator\", {\"ESPECIE\": cientifico_especie_trie})\n",
    "corte_annotator = GazetteerAnnotator(\"corte_annotator\", {\"CORTE\": corte_trie})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<skweak.base.CombinedAnnotator at 0x248555daf80>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = CombinedAnnotator()\n",
    "combined.add_annotator(especie_annotator)\n",
    "combined.add_annotator(cientifico_especie_annotator)\n",
    "combined.add_annotator(corte_annotator)\n",
    "combined.add_annotator(dimensao_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "madeira serrada massaranduba (manilkara huberi) 2.5 metro 2 vigas de 10x15cm e 3 tabuas de 2,5x30x200cm"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"madeira serrada massaranduba (manilkara huberi) 2.5 metro 2 vigas de 10x15cm e 3 tabuas de 2,5x30x200cm\")\n",
    "combined(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESPECIE: ['massaranduba']\n",
      "ESPECIE: ['manilkara huberi']\n",
      "CORTE: ['vigas', 'tabuas']\n",
      "DIMENSAO: ['2,5x30x200cm', '10x15cm', '2,5x30x200', '2.5 metro']\n"
     ]
    }
   ],
   "source": [
    "print(\"ESPECIE:\", [ent.text for ent in doc.spans[\"especie_annotator\"]])\n",
    "print(\"ESPECIE:\", [ent.text for ent in doc.spans[\"cientifico_especie_annotator\"]])\n",
    "print(\"CORTE:\", [ent.text for ent in doc.spans[\"corte_annotator\"]])\n",
    "print(\"DIMENSAO:\", [ent.text for ent in doc.spans[\"dimensao_annotator\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_docs_train = list(combined.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to artefatos/spacy_docs_train_annotated.bin...done\n"
     ]
    }
   ],
   "source": [
    "skweak.utils.docbin_writer(\n",
    "    spacy_docs_train, \"artefatos/spacy_docs_train_annotated.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skweak\n",
    "\n",
    "# Now we can load them back, so we don't need to run the labelling functions again\n",
    "spacy_docs_train = skweak.utils.docbin_reader(\n",
    "    \"artefatos/spacy_docs_train_annotated.bin\", spacy_model_name=\"pt_core_news_lg\"\n",
    ")\n",
    "spacy_docs_train = list(spacy_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_voter = skweak.aggregation.MajorityVoter(\n",
    "    name=\"doclevel_voter\",\n",
    "    labels=[\"ESPECIE\", \"CORTE\", \"DIMENSAO\"],\n",
    "    initial_weights={\"doc_majority\": 0.0}  # Exclude document-level annotator\n",
    ")\n",
    "\n",
    "# First apply base annotators and majority voting\n",
    "spacy_docs_train = list(majority_voter.pipe(spacy_docs_train))\n",
    "\n",
    "# 2. Create Document Majority Annotator\n",
    "doc_majority = skweak.doclevel.DocumentMajorityAnnotator(\"doc_majority\",\"doclevel_voter\")\n",
    "\n",
    "# Apply document-level consistency\n",
    "spacy_docs_train = list(doc_majority.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "madeira serrada massaranduba (manilkara huberi) 2.5 metro 2 vigas de 10x15cm e 3 tabuas de 2,5x30x200cm"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19902 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1 -131838.81912378             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19902 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2 -126730.80043565   +5108.01868813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19902 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3 -125252.04498185   +1478.75545379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Number of processed documents: 7000\n",
      "Number of processed documents: 8000\n",
      "Number of processed documents: 9000\n",
      "Number of processed documents: 10000\n",
      "Number of processed documents: 11000\n",
      "Number of processed documents: 12000\n",
      "Number of processed documents: 13000\n",
      "Number of processed documents: 14000\n",
      "Number of processed documents: 15000\n",
      "Number of processed documents: 16000\n",
      "Number of processed documents: 17000\n",
      "Number of processed documents: 18000\n",
      "Number of processed documents: 19000\n",
      "Finished E-step with 19902 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4 -124545.51782849    +706.52715336\n"
     ]
    }
   ],
   "source": [
    "initial_weights = {\n",
    "    \"especie_annotator\": .9,\n",
    "    \"cientifico_especie_annotator\": 1,\n",
    "    \"corte_annotator\": 1,\n",
    "    \"dimensao_annotator\": 1,\n",
    "    \"doclevel_voter\": 0.5,\n",
    "    \"doc_majority\": 0.6\n",
    "}\n",
    "\n",
    "hmm = skweak.generative.HMM(\n",
    "    \"hmm\", \n",
    "    labels=[\"ESPECIE\", \"CORTE\", \"DIMENSAO\"],\n",
    "    initial_weights=initial_weights\n",
    ")\n",
    "\n",
    "\n",
    "hmm.fit(spacy_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM model with following parameters:\n",
      "Output labels: ['O', 'B-ESPECIE', 'I-ESPECIE', 'B-CORTE', 'I-CORTE', 'B-DIMENSAO', 'I-DIMENSAO']\n",
      "--------\n",
      "Start distribution:\n",
      "O             0.49\n",
      "B-ESPECIE     0.16\n",
      "I-ESPECIE     0.00\n",
      "B-CORTE       0.35\n",
      "I-CORTE       0.00\n",
      "B-DIMENSAO    0.00\n",
      "I-DIMENSAO    0.00\n",
      "dtype: float64\n",
      "--------\n",
      "Transition model:\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.78       0.03       0.00     0.14      0.0        0.05        0.00\n",
      "B-ESPECIE   0.64       0.00       0.17     0.00      0.0        0.19        0.00\n",
      "I-ESPECIE   0.86       0.00       0.00     0.00      0.0        0.13        0.00\n",
      "B-CORTE     0.68       0.20       0.00     0.01      0.0        0.12        0.00\n",
      "I-CORTE     0.20       0.20       0.00     0.20      0.2        0.20        0.00\n",
      "B-DIMENSAO  0.02       0.00       0.00     0.00      0.0        0.01        0.97\n",
      "I-DIMENSAO  0.41       0.03       0.00     0.00      0.0        0.17        0.39\n",
      "--------\n",
      "Labelling functions in model: ['dimensao_annotator', 'especie_annotator', 'corte_annotator', 'doc_majority', 'cientifico_especie_annotator']\n",
      "Emission model for: cientifico_especie_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.90       0.06       0.04     0.00     0.00        0.00        0.00\n",
      "B-ESPECIE   0.01       0.98       0.00     0.00     0.00        0.00        0.00\n",
      "I-ESPECIE   0.00       0.00       1.00     0.00     0.00        0.00        0.00\n",
      "B-CORTE     1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-CORTE     0.23       0.00       0.00     0.00     0.77        0.00        0.00\n",
      "B-DIMENSAO  1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-DIMENSAO  0.99       0.01       0.00     0.00     0.00        0.00        0.00\n",
      "weights     0.95       0.96       1.00     0.93     0.93        0.93        0.93\n",
      "--------\n",
      "Emission model for: corte_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.99        0.0        0.0     0.01     0.00         0.0         0.0\n",
      "B-ESPECIE   1.00        0.0        0.0     0.00     0.00         0.0         0.0\n",
      "I-ESPECIE   1.00        0.0        0.0     0.00     0.00         0.0         0.0\n",
      "B-CORTE     0.00        0.0        0.0     1.00     0.00         0.0         0.0\n",
      "I-CORTE     0.23        0.0        0.0     0.00     0.77         0.0         0.0\n",
      "B-DIMENSAO  1.00        0.0        0.0     0.00     0.00         0.0         0.0\n",
      "I-DIMENSAO  0.99        0.0        0.0     0.01     0.00         0.0         0.0\n",
      "weights     1.00        1.0        1.0     1.00     1.00         1.0         1.0\n",
      "--------\n",
      "Emission model for: dimensao_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.90        0.0        0.0      0.0     0.00        0.09         0.0\n",
      "B-ESPECIE   1.00        0.0        0.0      0.0     0.00        0.00         0.0\n",
      "I-ESPECIE   1.00        0.0        0.0      0.0     0.00        0.00         0.0\n",
      "B-CORTE     1.00        0.0        0.0      0.0     0.00        0.00         0.0\n",
      "I-CORTE     0.23        0.0        0.0      0.0     0.77        0.00         0.0\n",
      "B-DIMENSAO  0.00        0.0        0.0      0.0     0.00        1.00         0.0\n",
      "I-DIMENSAO  0.00        0.0        0.0      0.0     0.00        0.00         1.0\n",
      "weights     1.00        1.0        1.0      1.0     1.00        1.00         1.0\n",
      "--------\n",
      "Emission model for: doc_majority\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.94       0.06       0.00     0.00     0.00        0.00        0.00\n",
      "B-ESPECIE   0.20       0.80       0.00     0.00     0.00        0.00        0.00\n",
      "I-ESPECIE   0.05       0.60       0.35     0.00     0.00        0.00        0.00\n",
      "B-CORTE     0.52       0.00       0.00     0.48     0.00        0.00        0.00\n",
      "I-CORTE     0.23       0.00       0.00     0.00     0.77        0.00        0.00\n",
      "B-DIMENSAO  0.76       0.00       0.00     0.00     0.00        0.24        0.00\n",
      "I-DIMENSAO  0.71       0.14       0.00     0.00     0.00        0.00        0.15\n",
      "weights     0.60       0.60       0.60     0.60     0.60        0.60        0.60\n",
      "--------\n",
      "Emission model for: especie_annotator\n",
      "               O  B-ESPECIE  I-ESPECIE  B-CORTE  I-CORTE  B-DIMENSAO  I-DIMENSAO\n",
      "O           0.90       0.10       0.00     0.00     0.00        0.00        0.00\n",
      "B-ESPECIE   0.00       1.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-ESPECIE   0.03       0.00       0.96     0.00     0.00        0.00        0.00\n",
      "B-CORTE     1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-CORTE     0.23       0.00       0.00     0.00     0.77        0.00        0.00\n",
      "B-DIMENSAO  1.00       0.00       0.00     0.00     0.00        0.00        0.00\n",
      "I-DIMENSAO  0.92       0.08       0.00     0.00     0.00        0.00        0.00\n",
      "weights     0.85       0.87       0.90     0.83     0.83        0.83        0.83\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "hmm.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_voter = skweak.voting.SequentialMajorityVoter(\n",
    "    \"maj_voter\",\n",
    "    labels=[\"ESPECIE\", \"CORTE\", \"DIMENSAO\"],\n",
    "    initial_weights=initial_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_docs_train = list(hmm.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_docs_train = list(maj_voter.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HMM Annotations ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7FFFD4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    angelim verm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " aproveitamento \n",
       "<mark class=\"entity\" style=\"background: #FFD700; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ripas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Majority Voter Annotations ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7FFFD4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    angelim verm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ESPECIE</span>\n",
       "</mark>\n",
       " aproveitamento \n",
       "<mark class=\"entity\" style=\"background: #FFD700; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ripas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CORTE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "def render_skweak_entities(doc, annotator_name: str, colors: dict = None):\n",
    "    \"\"\"Render entities from skweak annotations with colored highlighting\"\"\"\n",
    "    \n",
    "    # Get entities from specified annotator\n",
    "    entities = doc.spans.get(annotator_name, [])\n",
    "    \n",
    "    # Default color scheme for your entities\n",
    "    default_colors = {\n",
    "        \"ESPECIE\": \"#7FFFD4\",  # Aquamarine\n",
    "        \"CORTE\": \"#FFD700\",    # Gold\n",
    "        \"DIMENSAO\": \"#FFB6C1\"  # Pink\n",
    "    }\n",
    "    \n",
    "    # Ensure colors is never None\n",
    "    colors = colors or default_colors\n",
    "    \n",
    "    # Prepare entities for visualization\n",
    "    displacy_data = {\n",
    "        \"text\": doc.text,\n",
    "        \"ents\": [\n",
    "            {\n",
    "                \"start\": ent.start_char,\n",
    "                \"end\": ent.end_char,\n",
    "                \"label\": ent.label_\n",
    "            } for ent in entities\n",
    "        ],\n",
    "        \"title\": None\n",
    "    }\n",
    "\n",
    "    # Render with specified colors (ensure empty dict if no colors)\n",
    "    displacy.render(\n",
    "        [displacy_data],\n",
    "        style=\"ent\",\n",
    "        manual=True,\n",
    "        jupyter=True,\n",
    "        options={\"colors\": colors or {}}  # Key fix here\n",
    "    )\n",
    "\n",
    "# Usage\n",
    "doc = spacy_docs_train[0]\n",
    "\n",
    "print(\"=== HMM Annotations ===\")\n",
    "render_skweak_entities(doc, \"hmm\")\n",
    "\n",
    "\n",
    "print(\"\\n=== Majority Voter Annotations ===\")\n",
    "render_skweak_entities(doc, \"maj_voter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to artefatos/spacy_docs_train_annotated.bin...done\n"
     ]
    }
   ],
   "source": [
    "from skweak.utils import docbin_writer\n",
    "docbin_writer(spacy_docs_train, \"artefatos/spacy_docs_train_annotated.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazer analise dos que não tiveram nenhum tipo de anotação, também de conflitos para entender possíveis melhoras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a new model to using weak labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ser capaz de generalizar, além do regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: [('angelim verm', 'ESPECIE'), ('ripas', 'CORTE')]\n"
     ]
    }
   ],
   "source": [
    "from skweak.utils import docbin_reader\n",
    "\n",
    "# Load your unlabeled documents with weak annotations\n",
    "spacy_docs_train = list(docbin_reader(\"artefatos/spacy_docs_train_annotated.bin\", \n",
    "                                     spacy_model_name=\"pt_core_news_lg\"))\n",
    "\n",
    "# Verify annotations\n",
    "doc = spacy_docs_train[0]\n",
    "print(\"Entities:\", [(ent.text, ent.label_) for ent in doc.spans[\"hmm\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_iob(docs, annotator_name=\"hmm\"):\n",
    "    \"\"\"Convert skweak annotations to IOB format\"\"\"\n",
    "    iob_data = []\n",
    "    for doc in docs:\n",
    "        tokens = [token.text for token in doc]\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "        \n",
    "        # Process each entity span\n",
    "        for span in doc.spans[annotator_name]:\n",
    "            label = span.label_\n",
    "            start = span.start\n",
    "            end = span.end\n",
    "            \n",
    "            labels[start] = f\"B-{label}\"\n",
    "            for i in range(start+1, end):\n",
    "                labels[i] = f\"I-{label}\"\n",
    "                \n",
    "        iob_data.append({\"tokens\": tokens, \"labels\": labels, \"text\": doc.text})\n",
    "    \n",
    "    return iob_data\n",
    "\n",
    "# Convert all training docs\n",
    "train_data = convert_to_iob(spacy_docs_train, annotator_name=\"hmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_argilla_to_iob(argilla_data):\n",
    "    \"\"\"Convert Argilla format to IOB format\"\"\"\n",
    "    iob_data = []\n",
    "    \n",
    "    for example in argilla_data:\n",
    "        text = example[\"text\"]\n",
    "        entities = example.get(\"span_label.responses\", [])\n",
    "        \n",
    "        if isinstance(entities, str):\n",
    "            try:\n",
    "                clean_str = entities.replace(\"array(\", \"\").replace(\", dtype=object)\", \"\")\n",
    "                entities = ast.literal_eval(clean_str)\n",
    "            except:\n",
    "                entities = []\n",
    "\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "\n",
    "        for entity in entities:\n",
    "            if not isinstance(entity, dict):\n",
    "                continue\n",
    "\n",
    "            start = entity[\"start\"]\n",
    "            end = entity[\"end\"]\n",
    "            label = entity[\"label\"]\n",
    "\n",
    "            start_token = None\n",
    "            for token in doc:\n",
    "                if token.idx >= start:\n",
    "                    break\n",
    "                start_token = token.i\n",
    "            if start_token is None:\n",
    "                start_token = 0\n",
    "\n",
    "            end_token = None\n",
    "            for token in doc:\n",
    "                if token.idx + len(token) > end:\n",
    "                    break\n",
    "                end_token = token.i\n",
    "            if end_token is None:\n",
    "                end_token = len(doc) - 1\n",
    "\n",
    "            if start_token <= end_token:\n",
    "                labels[start_token] = f\"B-{label}\"\n",
    "                for i in range(start_token + 1, end_token + 1):\n",
    "                    labels[i] = f\"I-{label}\"\n",
    "\n",
    "        iob_data.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"labels\": labels,\n",
    "            \"text\": text\n",
    "        })\n",
    "    \n",
    "    return iob_data\n",
    "\n",
    "val_data = convert_argilla_to_iob(df_val.to_dict(\"records\"))\n",
    "test_data = convert_argilla_to_iob(df_test.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 21010/21010 [00:03<00:00, 5959.07 examples/s]\n",
      "Casting the dataset: 100%|██████████| 282/282 [00:00<00:00, 51101.43 examples/s]\n",
      "Casting the dataset: 100%|██████████| 282/282 [00:00<00:00, 56393.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Sequence, ClassLabel, Value\n",
    "\n",
    "labels = [\"O\", \"B-ESPECIE\", \"I-ESPECIE\", \"B-CORTE\", \"I-CORTE\", \"B-DIMENSAO\", \"I-DIMENSAO\"]\n",
    "\n",
    "# Create dataset\n",
    "dataset_train = Dataset.from_list(train_data).cast(Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    \"text\": Value(\"string\")\n",
    "}))\n",
    "dataset_val = Dataset.from_list(val_data).cast(Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    \"text\": Value(\"string\")\n",
    "}))\n",
    "dataset_test = Dataset.from_list(test_data).cast(Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    \"text\": Value(\"string\")\n",
    "}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 21010/21010 [00:01<00:00, 10839.58 examples/s]\n",
      "Map: 100%|██████████| 282/282 [00:00<00:00, 10941.76 examples/s]\n",
      "Map: 100%|██████████| 282/282 [00:00<00:00, 10888.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset_train = dataset_train.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_dataset_val = dataset_val.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_dataset_test = dataset_test.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, max_length=512, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3285' max='3285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3285/3285 12:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.388400</td>\n",
       "      <td>1.651983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>1.946672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.191900</td>\n",
       "      <td>2.135130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>2.207854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>2.236130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tales\\Python\\Madeira-Fraca\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3285, training_loss=0.19342311534355855, metrics={'train_runtime': 761.1559, 'train_samples_per_second': 138.014, 'train_steps_per_second': 4.316, 'total_flos': 1688945476785864.0, 'train_loss': 0.19342311534355855, 'epoch': 5.0})"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"neuralmind/bert-base-portuguese-cased\",\n",
    "    num_labels=len(labels),\n",
    "    id2label={i: label for i, label in enumerate(labels)},\n",
    "    label2id={label: i for i, label in enumerate(labels)}\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./artefatos/bert-base-ner-true-labels\",  # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    per_device_train_batch_size=32,  # Batch size for training\n",
    "    per_device_eval_batch_size=32,  # Batch size for evaluation\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    "    bf16=True,  # Use bfloat16 precision for training (if supported by hardware)\n",
    "    fp16=False,  # Use mixed precision training with FP16 (if supported by hardware)\n",
    "    save_total_limit=1,  # Limit the total number of saved checkpoints\n",
    "    logging_steps=1,  # Log training metrics every step\n",
    "    eval_steps=1,  # Evaluate the model every step\n",
    "    save_steps=1,  # Save the model every step\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to determine the best model\n",
    "    greater_is_better=False,  # Higher metric value is better\n",
    "    logging_strategy=\"steps\",  # Log metrics at each step\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    do_train=True,  # Perform training\n",
    "    do_eval=True,  # Perform evaluation\n",
    "    gradient_accumulation_steps=1,  # Accumulate gradients over multiple steps\n",
    "    push_to_hub=False,  # Do not push the model to the Hugging Face Hub\n",
    "    learning_rate=2e-6,  # Learning rate for the optimizer\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities detected:\n",
      "vig -> CORTE\n",
      "10x -> DIMENSAO\n",
      "##15cm -> DIMENSAO\n",
      "tabu -> CORTE\n",
      "2, -> DIMENSAO\n",
      "5x -> DIMENSAO\n",
      "##30x -> DIMENSAO\n",
      "##200c -> DIMENSAO\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"madeira serrada massaranduba (manilkara huberi) 2 vigas de 10x15cm e 3 tabuas de 2,5x30x200cm\"\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "print(\"Entities detected:\")\n",
    "for entity in results:\n",
    "    print(f\"{entity['word']} -> {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problemas tokenizer não é o mesmo, faz sentido quebrar tanto nesse, pensar em workarounds, ou em formas de treinar que sirvam, pra isso.<br>\n",
    "voltar a olhar o que o professor para me inspirar, pq tambem foi com base em palavra."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
